{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-IV Data Exploration\n",
    "\n",
    "This notebook performs Exploratory Data Analysis (EDA) on MIMIC-IV data to understand patient statistics, feature distributions, and dataset complexity. The analysis includes patient demographics, admission patterns, laboratory events, microbiology tests, and chart events (vitals).\n",
    "\n",
    "**Important Note**: The `chartevents` and `labevents` tables are massive. This notebook uses chunking to avoid Out-Of-Memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Set plot style for better readability\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths - update these to match your data location\n",
    "patients_file = 'patients.csv.gz'\n",
    "admissions_file = 'admissions.csv.gz'\n",
    "labevents_file = 'labevents.csv.gz'\n",
    "d_labitems_file = 'd_labitems.csv.gz'\n",
    "microbiologyevents_file = 'microbiologyevents.csv.gz'\n",
    "chartevents_file = 'chartevents.csv.gz'\n",
    "d_items_file = 'd_items.csv.gz'\n",
    "\n",
    "print(\"File paths defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Patient & Admission Statistics\n",
    "\n",
    "Load patient and admission data to understand the overall structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patients table\n",
    "print(\"Loading patients table...\")\n",
    "patients_df = pd.read_csv(patients_file)\n",
    "print(f\"Patients loaded: {len(patients_df)} rows\")\n",
    "print(f\"Columns: {list(patients_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load admissions table\n",
    "print(\"Loading admissions table...\")\n",
    "admissions_df = pd.read_csv(admissions_file)\n",
    "print(f\"Admissions loaded: {len(admissions_df)} rows\")\n",
    "print(f\"Columns: {list(admissions_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics\n",
    "total_unique_patients = patients_df['subject_id'].nunique()\n",
    "total_admissions = len(admissions_df)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PATIENT & ADMISSION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Unique Patients: {total_unique_patients:,}\")\n",
    "print(f\"Total Admissions: {total_admissions:,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate admissions per patient\n",
    "admissions_per_patient = admissions_df.groupby('subject_id').size()\n",
    "\n",
    "min_admissions = admissions_per_patient.min()\n",
    "max_admissions = admissions_per_patient.max()\n",
    "avg_admissions = admissions_per_patient.mean()\n",
    "\n",
    "print(\"ADMISSIONS PER PATIENT:\")\n",
    "print(f\"  Minimum: {min_admissions}\")\n",
    "print(f\"  Maximum: {max_admissions}\")\n",
    "print(f\"  Average: {avg_admissions:.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Histogram of admissions per patient\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(admissions_per_patient, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Admissions per Patient')\n",
    "plt.ylabel('Frequency (Number of Patients)')\n",
    "plt.title('Distribution of Number of Admissions per Patient')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Laboratory Events Exploration\n",
    "\n",
    "Analyze laboratory test events. The `labevents` table is large, so we use chunk processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load d_labitems dictionary for mapping itemid to label\n",
    "print(\"Loading d_labitems dictionary...\")\n",
    "d_labitems_df = pd.read_csv(d_labitems_file)\n",
    "print(f\"Total lab items in dictionary: {len(d_labitems_df)}\")\n",
    "\n",
    "# Create mapping from itemid to label\n",
    "labitem_map = dict(zip(d_labitems_df['itemid'], d_labitems_df['label']))\n",
    "print(f\"Mapping created for {len(labitem_map)} lab items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labevents in chunks and aggregate\n",
    "print(\"\\nReading labevents file in chunks...\")\n",
    "chunk_size = 1000000\n",
    "lab_events_per_admission = defaultdict(int)\n",
    "itemid_frequency = defaultdict(int)\n",
    "chunk_number = 0\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    labevents_file,\n",
    "    usecols=['hadm_id', 'itemid'],\n",
    "    chunksize=chunk_size\n",
    "):\n",
    "    chunk_number += 1\n",
    "    print(f\"Processing chunk {chunk_number}... ({len(chunk):,} rows)\")\n",
    "    \n",
    "    # Remove rows with missing hadm_id\n",
    "    chunk = chunk.dropna(subset=['hadm_id'])\n",
    "    \n",
    "    # Count lab events per admission\n",
    "    for hadm_id in chunk['hadm_id']:\n",
    "        lab_events_per_admission[hadm_id] += 1\n",
    "    \n",
    "    # Count frequency of each itemid\n",
    "    itemid_counts = chunk['itemid'].value_counts()\n",
    "    for itemid, count in itemid_counts.items():\n",
    "        itemid_frequency[itemid] += count\n",
    "\n",
    "print(f\"\\nTotal chunks processed: {chunk_number}\")\n",
    "print(f\"Total admissions with lab events: {len(lab_events_per_admission):,}\")\n",
    "print(f\"Unique lab test types: {len(itemid_frequency):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for lab events per admission\n",
    "lab_events_counts = list(lab_events_per_admission.values())\n",
    "min_lab_events = min(lab_events_counts)\n",
    "max_lab_events = max(lab_events_counts)\n",
    "avg_lab_events = np.mean(lab_events_counts)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LAB EVENTS PER ADMISSION:\")\n",
    "print(f\"  Minimum: {min_lab_events}\")\n",
    "print(f\"  Maximum: {max_lab_events}\")\n",
    "print(f\"  Average: {avg_lab_events:.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Histogram of lab events per admission\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(lab_events_counts, bins=100, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Lab Events per Admission')\n",
    "plt.ylabel('Frequency (Number of Admissions)')\n",
    "plt.title('Distribution of Number of Lab Events per Admission')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 20 most frequent lab tests\n",
    "top_20_labitems = sorted(itemid_frequency.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Map itemids to labels\n",
    "top_20_labels = []\n",
    "top_20_counts = []\n",
    "for itemid, count in top_20_labitems:\n",
    "    label = labitem_map.get(itemid, f\"Unknown ({itemid})\")\n",
    "    top_20_labels.append(label)\n",
    "    top_20_counts.append(count)\n",
    "\n",
    "print(\"\\nTOP 20 MOST FREQUENT LAB TESTS:\")\n",
    "for i, (label, count) in enumerate(zip(top_20_labels, top_20_counts), 1):\n",
    "    print(f\"{i:2d}. {label:50s} - {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Horizontal bar chart of top 20 lab tests\n",
    "plt.figure(figsize=(12, 8))\n",
    "y_pos = np.arange(len(top_20_labels))\n",
    "plt.barh(y_pos, top_20_counts, alpha=0.7)\n",
    "plt.yticks(y_pos, top_20_labels)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Lab Test')\n",
    "plt.title('Top 20 Most Frequent Laboratory Tests')\n",
    "plt.gca().invert_yaxis()  # Highest at the top\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Laboratory Test Repetition Frequency Analysis\n",
    "\n",
    "Analyze how frequently each laboratory test is repeated per admission. This analysis helps distinguish between static features (measured once) and time-series features (measured repeatedly), which is essential for the P-CAFE/MDP framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze repetition frequency of laboratory tests per admission\n",
    "print(\"\\nAnalyzing laboratory test repetition frequency...\")\n",
    "print(\"This analysis counts how many times each specific test (itemid) appears within a single admission (hadm_id).\\n\")\n",
    "\n",
    "# Initialize a Counter to store counts for each (hadm_id, itemid) pair\n",
    "repetition_counts = Counter()\n",
    "chunk_size = 1000000\n",
    "chunk_number = 0\n",
    "\n",
    "print(\"Reading labevents file in chunks...\")\n",
    "for chunk in pd.read_csv(\n",
    "    labevents_file,\n",
    "    usecols=['hadm_id', 'itemid'],\n",
    "    chunksize=chunk_size\n",
    "):\n",
    "    chunk_number += 1\n",
    "    print(f\"Processing chunk {chunk_number}... ({len(chunk):,} rows)\")\n",
    "    \n",
    "    # Filter out rows with missing hadm_id (outpatient/unlinked data)\n",
    "    chunk = chunk.dropna(subset=['hadm_id'])\n",
    "    \n",
    "    # Group by (hadm_id, itemid) and count occurrences\n",
    "    chunk_counts = chunk.groupby(['hadm_id', 'itemid']).size()\n",
    "    \n",
    "    # Accumulate counts across chunks using Counter.update()\n",
    "    repetition_counts.update(chunk_counts)\n",
    "\n",
    "print(f\"\\nTotal chunks processed: {chunk_number}\")\n",
    "print(f\"Total unique (admission, test) pairs: {len(repetition_counts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the repetition counts\n",
    "print(\"\\nCreating repetition_stats DataFrame...\")\n",
    "repetition_stats = pd.DataFrame([\n",
    "    {'hadm_id': hadm_id, 'itemid': itemid, 'count': count}\n",
    "    for (hadm_id, itemid), count in repetition_counts.items()\n",
    "])\n",
    "\n",
    "print(f\"DataFrame created with {len(repetition_stats):,} rows\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(repetition_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for repetitions per test per admission\n",
    "mean_repetitions = repetition_stats['count'].mean()\n",
    "median_repetitions = repetition_stats['count'].median()\n",
    "max_repetitions = repetition_stats['count'].max()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LABORATORY TEST REPETITION FREQUENCY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mean repetitions per test per admission:   {mean_repetitions:.2f}\")\n",
    "print(f\"Median repetitions per test per admission: {median_repetitions:.0f}\")\n",
    "print(f\"Max repetitions per test per admission:    {max_repetitions}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Histogram of repetitions per test per admission\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(repetition_stats['count'], bins=100, edgecolor='black', alpha=0.7, color='mediumseagreen')\n",
    "plt.xlabel('Number of Repetitions per Test per Admission')\n",
    "plt.ylabel('Frequency (Log Scale)')\n",
    "plt.title('Distribution of Laboratory Test Repetition Frequency per Admission')\n",
    "plt.yscale('log')  # Use log scale for Y-axis due to highly skewed distribution\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top 20 tests with highest average repetition count\n",
    "print(\"\\nCalculating average repetitions per test (itemid)...\")\n",
    "avg_repetitions_per_test = repetition_stats.groupby('itemid')['count'].mean().sort_values(ascending=False)\n",
    "top_20_repeated = avg_repetitions_per_test.head(20)\n",
    "\n",
    "# Join with d_labitems to get test names\n",
    "top_20_repeated_df = pd.DataFrame({\n",
    "    'itemid': top_20_repeated.index,\n",
    "    'avg_repetitions': top_20_repeated.values\n",
    "})\n",
    "\n",
    "# Merge with d_labitems to get labels\n",
    "top_20_repeated_df = top_20_repeated_df.merge(\n",
    "    d_labitems_df[['itemid', 'label']], \n",
    "    on='itemid', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing labels\n",
    "top_20_repeated_df['label'] = top_20_repeated_df.apply(\n",
    "    lambda row: row['label'] if pd.notna(row['label']) else f\"Unknown ({int(row['itemid'])})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nTOP 20 LABORATORY TESTS WITH HIGHEST AVERAGE REPETITION COUNT:\")\n",
    "print(\"=\" * 80)\n",
    "for i, row in top_20_repeated_df.iterrows():\n",
    "    print(f\"{i+1:2d}. {row['label']:50s} - Avg: {row['avg_repetitions']:.2f} repetitions/admission\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar chart of top 20 tests by average repetition count\n",
    "plt.figure(figsize=(12, 8))\n",
    "y_pos = np.arange(len(top_20_repeated_df))\n",
    "plt.barh(y_pos, top_20_repeated_df['avg_repetitions'], alpha=0.7, color='mediumseagreen')\n",
    "plt.yticks(y_pos, top_20_repeated_df['label'])\n",
    "plt.xlabel('Average Number of Repetitions per Admission')\n",
    "plt.ylabel('Laboratory Test')\n",
    "plt.title('Top 20 Laboratory Tests by Average Repetition Count per Admission')\n",
    "plt.gca().invert_yaxis()  # Highest at the top\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Microbiology Events Exploration\n",
    "\n",
    "Analyze microbiology test events including cultures and susceptibility tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load microbiologyevents table\n",
    "print(\"Loading microbiologyevents table...\")\n",
    "micro_df = pd.read_csv(microbiologyevents_file)\n",
    "print(f\"Microbiology events loaded: {len(micro_df):,} rows\")\n",
    "print(f\"Columns: {list(micro_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing hadm_id\n",
    "micro_df = micro_df.dropna(subset=['hadm_id'])\n",
    "print(f\"After removing missing hadm_id: {len(micro_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count microbiology events per admission\n",
    "micro_events_per_admission = micro_df.groupby('hadm_id').size()\n",
    "\n",
    "min_micro_events = micro_events_per_admission.min()\n",
    "max_micro_events = micro_events_per_admission.max()\n",
    "avg_micro_events = micro_events_per_admission.mean()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MICROBIOLOGY EVENTS PER ADMISSION:\")\n",
    "print(f\"  Minimum: {min_micro_events}\")\n",
    "print(f\"  Maximum: {max_micro_events}\")\n",
    "print(f\"  Average: {avg_micro_events:.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Histogram of microbiology events per admission\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(micro_events_per_admission, bins=100, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Microbiology Events per Admission')\n",
    "plt.ylabel('Frequency (Number of Admissions)')\n",
    "plt.title('Distribution of Number of Microbiology Events per Admission')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined test name from test_name and spec_type_desc\n",
    "micro_df['combined_test'] = micro_df.apply(\n",
    "    lambda row: f\"{row['test_name']} - {row['spec_type_desc']}\" \n",
    "    if pd.notna(row['spec_type_desc']) and pd.notna(row['test_name'])\n",
    "    else (row['test_name'] if pd.notna(row['test_name']) else 'Unknown'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Count frequency of each test\n",
    "test_frequency = micro_df['combined_test'].value_counts()\n",
    "top_20_micro_tests = test_frequency.head(20)\n",
    "\n",
    "print(\"\\nTOP 20 MOST FREQUENT MICROBIOLOGY TESTS:\")\n",
    "for i, (test_name, count) in enumerate(top_20_micro_tests.items(), 1):\n",
    "    print(f\"{i:2d}. {test_name:60s} - {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Horizontal bar chart of top 20 microbiology tests\n",
    "plt.figure(figsize=(12, 8))\n",
    "y_pos = np.arange(len(top_20_micro_tests))\n",
    "plt.barh(y_pos, top_20_micro_tests.values, alpha=0.7, color='coral')\n",
    "plt.yticks(y_pos, top_20_micro_tests.index)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Microbiology Test')\n",
    "plt.title('Top 20 Most Frequent Microbiology Tests')\n",
    "plt.gca().invert_yaxis()  # Highest at the top\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Chart Events (Vitals) Exploration\n",
    "\n",
    "Analyze chart events which contain vital signs and nursing observations. This is the largest table in MIMIC-IV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load d_items dictionary for mapping itemid to label\n",
    "print(\"Loading d_items dictionary...\")\n",
    "d_items_df = pd.read_csv(d_items_file)\n",
    "print(f\"Total chart items in dictionary: {len(d_items_df)}\")\n",
    "\n",
    "# Create mapping from itemid to label\n",
    "chartitem_map = dict(zip(d_items_df['itemid'], d_items_df['label']))\n",
    "print(f\"Mapping created for {len(chartitem_map)} chart items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read chartevents in chunks and count itemid frequency\n",
    "print(\"\\nReading chartevents file in chunks (this may take a while)...\")\n",
    "chunk_size = 1000000\n",
    "chart_itemid_frequency = defaultdict(int)\n",
    "chunk_number = 0\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    chartevents_file,\n",
    "    usecols=['itemid'],\n",
    "    chunksize=chunk_size\n",
    "):\n",
    "    chunk_number += 1\n",
    "    print(f\"Processing chunk {chunk_number}... ({len(chunk):,} rows)\")\n",
    "    \n",
    "    # Count frequency of each itemid\n",
    "    itemid_counts = chunk['itemid'].value_counts()\n",
    "    for itemid, count in itemid_counts.items():\n",
    "        chart_itemid_frequency[itemid] += count\n",
    "\n",
    "print(f\"\\nTotal chunks processed: {chunk_number}\")\n",
    "print(f\"Unique chart event types: {len(chart_itemid_frequency):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 20 most frequent chart events\n",
    "top_20_chartitems = sorted(chart_itemid_frequency.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Map itemids to labels\n",
    "top_20_chart_labels = []\n",
    "top_20_chart_counts = []\n",
    "for itemid, count in top_20_chartitems:\n",
    "    label = chartitem_map.get(itemid, f\"Unknown ({itemid})\")\n",
    "    top_20_chart_labels.append(label)\n",
    "    top_20_chart_counts.append(count)\n",
    "\n",
    "print(\"\\nTOP 20 MOST FREQUENT CHART EVENTS (VITALS/OBSERVATIONS):\")\n",
    "for i, (label, count) in enumerate(zip(top_20_chart_labels, top_20_chart_counts), 1):\n",
    "    print(f\"{i:2d}. {label:50s} - {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Horizontal bar chart of top 20 chart events\n",
    "plt.figure(figsize=(12, 8))\n",
    "y_pos = np.arange(len(top_20_chart_labels))\n",
    "plt.barh(y_pos, top_20_chart_counts, alpha=0.7, color='steelblue')\n",
    "plt.yticks(y_pos, top_20_chart_labels)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Chart Event (Vital/Observation)')\n",
    "plt.title('Top 20 Most Frequent Chart Events')\n",
    "plt.gca().invert_yaxis()  # Highest at the top\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully performed comprehensive exploratory data analysis on MIMIC-IV data:\n",
    "\n",
    "1. ✅ **Setup & Configuration**: Imported pandas, matplotlib, seaborn; set readable plot style; defined file paths\n",
    "2. ✅ **Patient & Admission Statistics**: Calculated total patients, admissions, and admissions per patient; created histogram\n",
    "3. ✅ **Laboratory Events**: Used chunk processing to analyze lab events; calculated statistics; created 2 visualizations (histogram and top 20 bar chart)\n",
    "4. ✅ **Microbiology Events**: Analyzed microbiology tests; calculated statistics; created 2 visualizations (histogram and top 20 bar chart)\n",
    "5. ✅ **Chart Events (Vitals)**: Used chunk processing to analyze the largest table; created top 20 visualization\n",
    "\n",
    "All visualizations include proper titles, axis labels, and are sized appropriately for readability. The chunking strategy successfully handles the massive `labevents` and `chartevents` tables without memory issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
