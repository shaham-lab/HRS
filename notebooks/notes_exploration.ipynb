{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MIMIC-IV Clinical Notes Exploration\n",
    "\n",
    "This notebook explores the structure and content of MIMIC-IV clinical notes to inform model selection for text processing. We analyze:\n",
    "- **Discharge summaries** and **Radiology reports**\n",
    "- Text length distributions to determine if we need standard BERT (512 tokens) or Longformer/ModernBERT (8k+ tokens)\n",
    "- Linkage between text files and metadata (_detail) files\n",
    "\n",
    "**Context**: Preparing unstructured clinical text for an RL agent (P-CAFE)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 1: Setup & Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style for better readability\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 2: Define File Paths\n",
    "\n",
    "Update these paths to match your MIMIC-IV data location. The files should be in the `note` module of MIMIC-IV."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define file paths - Update these to match your data location\n",
    "# Standard MIMIC-IV structure: physionet.org/files/mimiciv/3.1/note/\n",
    "\n",
    "# Adjust this base path to your MIMIC-IV data directory\n",
    "base_path = 'C:\\\\Users\\\\Eli\\\\Data\\\\physionet.org\\\\files\\\\mimic-iv-note\\\\2.2\\\\note\\\\'\n",
    "\n",
    "# Clinical text files\n",
    "discharge_file = base_path + 'discharge.csv.gz'\n",
    "radiology_file = base_path + 'radiology.csv.gz'\n",
    "\n",
    "# Metadata files\n",
    "discharge_detail_file = base_path + 'discharge_detail.csv.gz'\n",
    "radiology_detail_file = base_path + 'radiology_detail.csv.gz'\n",
    "\n",
    "print(\"File paths defined.\")\n",
    "print(f\"Discharge: {discharge_file}\")\n",
    "print(f\"Discharge Detail: {discharge_detail_file}\")\n",
    "print(f\"Radiology: {radiology_file}\")\n",
    "print(f\"Radiology Detail: {radiology_detail_file}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 3: Load Data (Optimized)\n",
    "\n",
    "Since note files contain large text fields, we load only the first 1,000 rows of each file to inspect structure without memory issues."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load discharge notes (first 1000 rows)\n",
    "print(\"Loading discharge notes (first 1000 rows)...\")\n",
    "discharge_df = pd.read_csv(discharge_file, nrows=1000, compression='gzip')\n",
    "print(f\"Discharge notes loaded: {len(discharge_df)} rows\")\n",
    "print(f\"Columns: {list(discharge_df.columns)}\")\n",
    "print(f\"Data types:\\n{discharge_df.dtypes}\")\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load discharge detail (first 1000 rows)\n",
    "print(\"Loading discharge detail (first 1000 rows)...\")\n",
    "discharge_detail_df = pd.read_csv(discharge_detail_file, nrows=1000, compression='gzip')\n",
    "print(f\"Discharge detail loaded: {len(discharge_detail_df)} rows\")\n",
    "print(f\"Columns: {list(discharge_detail_df.columns)}\")\n",
    "print(f\"Data types:\\n{discharge_detail_df.dtypes}\")\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load radiology notes (first 1000 rows)\n",
    "print(\"Loading radiology notes (first 1000 rows)...\")\n",
    "radiology_df = pd.read_csv(radiology_file, nrows=1000, compression='gzip')\n",
    "print(f\"Radiology notes loaded: {len(radiology_df)} rows\")\n",
    "print(f\"Columns: {list(radiology_df.columns)}\")\n",
    "print(f\"Data types:\\n{radiology_df.dtypes}\")\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load radiology detail (first 1000 rows)\n",
    "print(\"Loading radiology detail (first 1000 rows)...\")\n",
    "radiology_detail_df = pd.read_csv(radiology_detail_file, nrows=1000, compression='gzip')\n",
    "print(f\"Radiology detail loaded: {len(radiology_detail_df)} rows\")\n",
    "print(f\"Columns: {list(radiology_detail_df.columns)}\")\n",
    "print(f\"Data types:\\n{radiology_detail_df.dtypes}\")\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: Structure Analysis - Linkage Check\n",
    "\n",
    "Verify if `note_id`, `subject_id`, and `hadm_id` exist in both text files and detail files. Check for one-to-one relationships."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check common identifiers in discharge files\n",
    "print(\"=== DISCHARGE FILES LINKAGE ===\")\n",
    "print(f\"Discharge columns: {list(discharge_df.columns)}\")\n",
    "print(f\"Discharge Detail columns: {list(discharge_detail_df.columns)}\")\n",
    "print()\n",
    "\n",
    "# Check for common columns\n",
    "discharge_common_cols = set(discharge_df.columns).intersection(set(discharge_detail_df.columns))\n",
    "print(f\"Common columns between discharge and discharge_detail: {discharge_common_cols}\")\n",
    "print()\n",
    "\n",
    "# If note_id exists, check for one-to-one relationship\n",
    "if 'note_id' in discharge_df.columns and 'note_id' in discharge_detail_df.columns:\n",
    "    discharge_note_ids = set(discharge_df['note_id'])\n",
    "    discharge_detail_note_ids = set(discharge_detail_df['note_id'])\n",
    "    \n",
    "    # Check intersection\n",
    "    common_note_ids = discharge_note_ids.intersection(discharge_detail_note_ids)\n",
    "    print(f\"Total note_ids in discharge: {len(discharge_note_ids)}\")\n",
    "    print(f\"Total note_ids in discharge_detail: {len(discharge_detail_note_ids)}\")\n",
    "    print(f\"Common note_ids: {len(common_note_ids)}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    discharge_duplicates = discharge_df['note_id'].duplicated().sum()\n",
    "    detail_duplicates = discharge_detail_df['note_id'].duplicated().sum()\n",
    "    print(f\"Duplicate note_ids in discharge: {discharge_duplicates}\")\n",
    "    print(f\"Duplicate note_ids in discharge_detail: {detail_duplicates}\")\n",
    "    \n",
    "    if discharge_duplicates == 0 and detail_duplicates == 0:\n",
    "        print(\"✓ One-to-one relationship confirmed for discharge files\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: Duplicate note_ids found, relationship may not be one-to-one\")\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check common identifiers in radiology files\n",
    "print(\"=== RADIOLOGY FILES LINKAGE ===\")\n",
    "print(f\"Radiology columns: {list(radiology_df.columns)}\")\n",
    "print(f\"Radiology Detail columns: {list(radiology_detail_df.columns)}\")\n",
    "print()\n",
    "\n",
    "# Check for common columns\n",
    "radiology_common_cols = set(radiology_df.columns).intersection(set(radiology_detail_df.columns))\n",
    "print(f\"Common columns between radiology and radiology_detail: {radiology_common_cols}\")\n",
    "print()\n",
    "\n",
    "# If note_id exists, check for one-to-one relationship\n",
    "if 'note_id' in radiology_df.columns and 'note_id' in radiology_detail_df.columns:\n",
    "    radiology_note_ids = set(radiology_df['note_id'])\n",
    "    radiology_detail_note_ids = set(radiology_detail_df['note_id'])\n",
    "    \n",
    "    # Check intersection\n",
    "    common_note_ids = radiology_note_ids.intersection(radiology_detail_note_ids)\n",
    "    print(f\"Total note_ids in radiology: {len(radiology_note_ids)}\")\n",
    "    print(f\"Total note_ids in radiology_detail: {len(radiology_detail_note_ids)}\")\n",
    "    print(f\"Common note_ids: {len(common_note_ids)}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    radiology_duplicates = radiology_df['note_id'].duplicated().sum()\n",
    "    detail_duplicates = radiology_detail_df['note_id'].duplicated().sum()\n",
    "    print(f\"Duplicate note_ids in radiology: {radiology_duplicates}\")\n",
    "    print(f\"Duplicate note_ids in radiology_detail: {detail_duplicates}\")\n",
    "    \n",
    "    if radiology_duplicates == 0 and detail_duplicates == 0:\n",
    "        print(\"✓ One-to-one relationship confirmed for radiology files\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: Duplicate note_ids found, relationship may not be one-to-one\")\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 5: Metadata Inspection\n",
    "\n",
    "Display unique values of `note_type` or `description` from the detail files to understand what kinds of reports are available."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inspect discharge detail metadata\n",
    "print(\"=== DISCHARGE DETAIL METADATA ===\")\n",
    "print(f\"Sample of discharge_detail:\")\n",
    "print(discharge_detail_df.head())\n",
    "print()\n",
    "\n",
    "# Check for note_type or description columns\n",
    "if 'note_type' in discharge_detail_df.columns:\n",
    "    print(f\"Unique note_type values in discharge_detail:\")\n",
    "    print(discharge_detail_df['note_type'].value_counts())\n",
    "    print()\n",
    "\n",
    "if 'description' in discharge_detail_df.columns:\n",
    "    print(f\"Unique description values in discharge_detail:\")\n",
    "    print(discharge_detail_df['description'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inspect radiology detail metadata\n",
    "print(\"=== RADIOLOGY DETAIL METADATA ===\")\n",
    "print(f\"Sample of radiology_detail:\")\n",
    "print(radiology_detail_df.head())\n",
    "print()\n",
    "\n",
    "# Check for note_type or description columns\n",
    "if 'note_type' in radiology_detail_df.columns:\n",
    "    print(f\"Unique note_type values in radiology_detail:\")\n",
    "    print(radiology_detail_df['note_type'].value_counts())\n",
    "    print()\n",
    "\n",
    "if 'description' in radiology_detail_df.columns:\n",
    "    print(f\"Unique description values in radiology_detail:\")\n",
    "    print(radiology_detail_df['description'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 6: Text Length Analysis (Crucial for Model Selection)\n",
    "\n",
    "Calculate word counts for each note to approximate token counts. This analysis will help determine whether we need:\n",
    "- **BERT**: Standard transformer (512 tokens max)\n",
    "- **Longformer/ModernBERT**: Extended context models (8k+ tokens)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate word counts for discharge notes\n",
    "print(\"=== DISCHARGE NOTES TEXT LENGTH ANALYSIS ===\")\n",
    "\n",
    "if 'text' in discharge_df.columns:\n",
    "    # Calculate word count (simple split on whitespace)\n",
    "    discharge_df['word_count'] = discharge_df['text'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_words = discharge_df['word_count'].mean()\n",
    "    median_words = discharge_df['word_count'].median()\n",
    "    max_words = discharge_df['word_count'].max()\n",
    "    percentile_95 = discharge_df['word_count'].quantile(0.95)\n",
    "    \n",
    "    print(f\"Word Count Statistics for Discharge Summaries:\")\n",
    "    print(f\"  Mean: {mean_words:.0f} words\")\n",
    "    print(f\"  Median: {median_words:.0f} words\")\n",
    "    print(f\"  Maximum: {max_words:.0f} words\")\n",
    "    print(f\"  95th Percentile: {percentile_95:.0f} words\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate percentage exceeding BERT limit (512 tokens ≈ 512 words as rough approximation)\n",
    "    bert_limit = 8192\n",
    "    exceeding_bert = (discharge_df['word_count'] > bert_limit).sum()\n",
    "    percent_exceeding = (exceeding_bert / len(discharge_df)) * 100\n",
    "    \n",
    "    print(f\"Notes exceeding Modern BERT limit (8192 words): {exceeding_bert}/{len(discharge_df)} ({percent_exceeding:.1f}%)\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Warning: 'text' column not found in discharge_df\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate word counts for radiology notes\n",
    "print(\"=== RADIOLOGY NOTES TEXT LENGTH ANALYSIS ===\")\n",
    "\n",
    "if 'text' in radiology_df.columns:\n",
    "    # Calculate word count (simple split on whitespace)\n",
    "    radiology_df['word_count'] = radiology_df['text'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_words = radiology_df['word_count'].mean()\n",
    "    median_words = radiology_df['word_count'].median()\n",
    "    max_words = radiology_df['word_count'].max()\n",
    "    percentile_95 = radiology_df['word_count'].quantile(0.95)\n",
    "    \n",
    "    print(f\"Word Count Statistics for Radiology Reports:\")\n",
    "    print(f\"  Mean: {mean_words:.0f} words\")\n",
    "    print(f\"  Median: {median_words:.0f} words\")\n",
    "    print(f\"  Maximum: {max_words:.0f} words\")\n",
    "    print(f\"  95th Percentile: {percentile_95:.0f} words\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate percentage exceeding BERT limit (512 tokens ≈ 512 words as rough approximation)\n",
    "    bert_limit = 4096\n",
    "    exceeding_bert = (radiology_df['word_count'] > bert_limit).sum()\n",
    "    percent_exceeding = (exceeding_bert / len(radiology_df)) * 100\n",
    "    \n",
    "    print(f\"Notes exceeding Modern BERT limit (8192 words): {exceeding_bert}/{len(radiology_df)} ({percent_exceeding:.1f}%)\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Warning: 'text' column not found in radiology_df\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 7: Visualization - Word Count Histograms\n",
    "\n",
    "Plot histograms of word counts with a vertical line at 512 words (BERT limit) to visualize how many notes would be truncated."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# BERT limit marker\n",
    "bert_limit = 512\n",
    "\n",
    "# Plot discharge notes word count histogram\n",
    "if 'word_count' in discharge_df.columns:\n",
    "    axes[0].hist(discharge_df['word_count'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(x=bert_limit, color='red', linestyle='--', linewidth=2, label=f'BERT Limit ({bert_limit} words)')\n",
    "    axes[0].set_xlabel('Word Count', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Discharge Summaries - Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No word_count data for discharge notes', \n",
    "                 ha='center', va='center', transform=axes[0].transAxes)\n",
    "\n",
    "# Plot radiology notes word count histogram\n",
    "if 'word_count' in radiology_df.columns:\n",
    "    axes[1].hist(radiology_df['word_count'], bins=50, color='darkorange', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(x=bert_limit, color='red', linestyle='--', linewidth=2, label=f'BERT Limit ({bert_limit} words)')\n",
    "    axes[1].set_xlabel('Word Count', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Radiology Reports - Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No word_count data for radiology notes', \n",
    "                 ha='center', va='center', transform=axes[1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Histogram visualization complete.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 8: Sample Display\n",
    "\n",
    "Display one full example of a discharge summary and one radiology report to inspect formatting, headers, and de-identification tags."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display a sample discharge summary\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE DISCHARGE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'text' in discharge_df.columns and len(discharge_df) > 0:\n",
    "    # Get the first non-empty text\n",
    "    sample_discharge = discharge_df[discharge_df['text'].notna()].iloc[0]\n",
    "    \n",
    "    print(f\"Note ID: {sample_discharge.get('note_id', 'N/A')}\")\n",
    "    print(f\"Subject ID: {sample_discharge.get('subject_id', 'N/A')}\")\n",
    "    print(f\"Hospital Admission ID: {sample_discharge.get('hadm_id', 'N/A')}\")\n",
    "    print(f\"Word Count: {sample_discharge.get('word_count', 'N/A')}\")\n",
    "    print()\n",
    "    print(\"Text Content:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(sample_discharge['text'])\n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No discharge text available to display.\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display a sample radiology report\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE RADIOLOGY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'text' in radiology_df.columns and len(radiology_df) > 0:\n",
    "    # Get the first non-empty text\n",
    "    sample_radiology = radiology_df[radiology_df['text'].notna()].iloc[0]\n",
    "    \n",
    "    print(f\"Note ID: {sample_radiology.get('note_id', 'N/A')}\")\n",
    "    print(f\"Subject ID: {sample_radiology.get('subject_id', 'N/A')}\")\n",
    "    print(f\"Hospital Admission ID: {sample_radiology.get('hadm_id', 'N/A')}\")\n",
    "    print(f\"Word Count: {sample_radiology.get('word_count', 'N/A')}\")\n",
    "    print()\n",
    "    print(\"Text Content:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(sample_radiology['text'])\n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No radiology text available to display.\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Embeddings for Clinical Notes"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Setup device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the Clinical ModernBERT model and tokenizer\n",
    "model_name = \"Simonlee711/Clinical_ModernBERT\"\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, add_pooling_layer=False).to(device)\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded successfully and moved to {device}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_note_embeddings(text_list, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of clinical notes using mean pooling.\n",
    "    \n",
    "    Args:\n",
    "        text_list: List of text strings to embed\n",
    "        batch_size: Number of texts to process in each batch\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings with shape (num_texts, embedding_dim)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = text_list[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=8192,\n",
    "            # Clinical_ModernBERT supports 8192 token context length\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move input tensors to device\n",
    "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "        \n",
    "        # Generate embeddings without gradient computation\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        # Extract last_hidden_state for mean pooling\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "        \n",
    "        # Perform mean pooling\n",
    "        # Expand attention_mask to match hidden_state dimensions\n",
    "        # attention_mask shape: (batch_size, sequence_length)\n",
    "        # We need: (batch_size, sequence_length, hidden_size)\n",
    "        attention_mask = encoded_input['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size())\n",
    "        \n",
    "        # Sum of token embeddings (masked by attention_mask)\n",
    "        sum_embeddings = torch.sum(last_hidden_state * attention_mask, dim=1)\n",
    "        \n",
    "        # Sum of attention mask (to get the count of real tokens)\n",
    "        sum_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-6)\n",
    "        \n",
    "        # Calculate mean by dividing sum of embeddings by sum of mask\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        batch_embeddings = mean_embeddings.cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batch embeddings\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check that discharge_df is loaded\n",
    "if 'discharge_df' not in locals():\n",
    "    raise ValueError(\"discharge_df is not loaded. Please run the data loading cells first.\")\n",
    "\n",
    "print(f\"discharge_df shape: {discharge_df.shape}\")\n",
    "print(f\"discharge_df columns: {list(discharge_df.columns)}\")\n",
    "\n",
    "# Extract text column as a list\n",
    "text_list = discharge_df['text'].fillna('').tolist()\n",
    "print(f\"\\nProcessing {len(text_list)} discharge notes...\")\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = get_note_embeddings(text_list, batch_size=8)\n",
    "print(f\"\\nEmbeddings generated with shape: {embeddings.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create DataFrame with required columns\n",
    "note_embeddings_df = pd.DataFrame({\n",
    "    'note_id': discharge_df['note_id'].values,\n",
    "    'subject_id': discharge_df['subject_id'].values,\n",
    "    'hadm_id': discharge_df['hadm_id'].values,\n",
    "    'note_embedding': list(embeddings)\n",
    "})\n",
    "\n",
    "print(f\"Created note_embeddings_df with shape: {note_embeddings_df.shape}\")\n",
    "print(f\"\\nColumns: {list(note_embeddings_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(note_embeddings_df.head())\n",
    "print(f\"\\nEmbedding vector size: {len(note_embeddings_df['note_embedding'].iloc[0])}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save to Parquet file\n",
    "output_file = 'discharge_notes_embeddings.parquet'\n",
    "note_embeddings_df.to_parquet(output_file, index=False)\n",
    "print(f\"Saved embeddings to {output_file}\")\n",
    "print(f\"Output DataFrame shape: {note_embeddings_df.shape}\")\n",
    "print(f\"Single embedding vector size: {len(note_embeddings_df['note_embedding'].iloc[0])}\")\n",
    "\n",
    "# Verify by loading the saved file\n",
    "print(f\"\\nVerifying saved file...\")\n",
    "loaded_df = pd.read_parquet(output_file)\n",
    "print(f\"Loaded DataFrame shape: {loaded_df.shape}\")\n",
    "print(f\"Loaded DataFrame columns: {list(loaded_df.columns)}\")\n",
    "print(f\"Loaded embedding vector size: {len(loaded_df['note_embedding'].iloc[0])}\")\n",
    "print(f\"\\nFirst few rows of loaded data:\")\n",
    "print(loaded_df.head())\n",
    "print(f\"\\n✓ File saved and verified successfully!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "1. **Model Selection**: Review the word count statistics and histogram visualizations to determine:\n",
    "   - If most notes are under 512 words → Standard BERT may be sufficient\n",
    "   - If many notes exceed 512 words → Consider Longformer or ModernBERT for longer context\n",
    "\n",
    "2. **Data Linkage**: The analysis shows how `note_id`, `subject_id`, and `hadm_id` link text files to metadata files.\n",
    "\n",
    "3. **Text Formatting**: Sample displays reveal the structure of clinical notes, including:\n",
    "   - De-identification tags (e.g., `[** ... **]`)\n",
    "   - Section headers\n",
    "   - Formatting conventions\n",
    "\n",
    "4. **Next Steps**:\n",
    "   - Update file paths in Section 2 to your actual MIMIC-IV data location\n",
    "   - Run the notebook to generate actual statistics\n",
    "   - Use the results to inform P-CAFE text processing strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
