{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-IV Clinical Notes Exploration\n",
    "\n",
    "This notebook explores the structure and content of MIMIC-IV clinical notes to inform model selection for text processing. We analyze:\n",
    "- **Discharge summaries** and **Radiology reports**\n",
    "- Text length distributions to determine if we need standard BERT (512 tokens) or Longformer/ModernBERT (8k+ tokens)\n",
    "- Linkage between text files and metadata (_detail) files\n",
    "\n",
    "**Context**: Preparing unstructured clinical text for an RL agent (P-CAFE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:46:55.689879100Z",
     "start_time": "2026-01-25T20:46:55.658499200Z"
    }
   },
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style for better readability\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Define File Paths\n",
    "\n",
    "Update these paths to match your MIMIC-IV data location. The files should be in the `note` module of MIMIC-IV."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:56:27.899514100Z",
     "start_time": "2026-01-25T20:56:27.876010100Z"
    }
   },
   "source": [
    "# Define file paths - Update these to match your data location\n",
    "# Standard MIMIC-IV structure: physionet.org/files/mimiciv/3.1/note/\n",
    "\n",
    "# Adjust this base path to your MIMIC-IV data directory\n",
    "base_path = 'C:\\\\Users\\\\Eli\\\\Data\\\\physionet.org\\\\files\\\\mimic-iv-note\\\\2.2\\\\Unzipped\\\\'\n",
    "\n",
    "# Clinical text files\n",
    "discharge_file = base_path + 'discharge.csv'\n",
    "radiology_file = base_path + 'radiology.csv'\n",
    "\n",
    "# Metadata files\n",
    "discharge_detail_file = base_path + 'discharge_detail.csv'\n",
    "radiology_detail_file = base_path + 'radiology_detail.csv'\n",
    "\n",
    "print(\"File paths defined.\")\n",
    "print(f\"Discharge: {discharge_file}\")\n",
    "print(f\"Discharge Detail: {discharge_detail_file}\")\n",
    "print(f\"Radiology: {radiology_file}\")\n",
    "print(f\"Radiology Detail: {radiology_detail_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File paths defined.\n",
      "Discharge: C:\\Users\\Eli\\Data\\physionet.org\\files\\mimic-iv-note\\2.2\\Unzipped\\discharge.csv\n",
      "Discharge Detail: C:\\Users\\Eli\\Data\\physionet.org\\files\\mimic-iv-note\\2.2\\Unzipped\\discharge_detail.csv\n",
      "Radiology: C:\\Users\\Eli\\Data\\physionet.org\\files\\mimic-iv-note\\2.2\\Unzipped\\radiology.csv\n",
      "Radiology Detail: C:\\Users\\Eli\\Data\\physionet.org\\files\\mimic-iv-note\\2.2\\Unzipped\\radiology_detail.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Full Dataset Statistics & Analysis\n",
    "\n",
    "This section analyzes the complete dataset using chunking to handle large files efficiently.\n",
    "We calculate statistics for:\n",
    "- Total number of records\n",
    "- Number of unique `subject_id` and `hadm_id`\n",
    "- Word count distributions for all notes\n",
    "\n",
    "**Note**: We use `pd.read_csv(..., chunksize=10000)` to iterate through the entire files without loading everything into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Starting full dataset analysis with chunking...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file_with_chunks(filepath, file_label, chunksize=10000):\n",
    "    \"\"\"\n",
    "    Analyze a large CSV file using chunking.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file (can be .csv or .csv.gz)\n",
    "        file_label: Label for printing (e.g., 'Discharge' or 'Radiology')\n",
    "        chunksize: Number of rows to read per chunk\n",
    "    \n",
    "    Returns:\n",
    "        dict with statistics: total_records, unique_subjects, unique_hadms, word_counts\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing {file_label}: {filepath}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_records = 0\n",
    "    unique_subjects = set()\n",
    "    unique_hadms = set()\n",
    "    word_counts = []  # Store only counts (integers) to save memory\n",
    "    \n",
    "    try:\n",
    "        # Read file in chunks\n",
    "        chunks = pd.read_csv(filepath, chunksize=chunksize, compression='gzip' if filepath.endswith('.gz') else None)\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunks, 1):\n",
    "            # Update total records\n",
    "            total_records += len(chunk)\n",
    "            \n",
    "            # Update unique IDs\n",
    "            if 'subject_id' in chunk.columns:\n",
    "                unique_subjects.update(chunk['subject_id'].dropna().unique())\n",
    "            if 'hadm_id' in chunk.columns:\n",
    "                unique_hadms.update(chunk['hadm_id'].dropna().unique())\n",
    "            \n",
    "            # Calculate word counts for text column\n",
    "            if 'text' in chunk.columns:\n",
    "                # Calculate word count for each note and store only the count\n",
    "                chunk_word_counts = chunk['text'].fillna('').apply(lambda x: len(str(x).split())).tolist()\n",
    "                word_counts.extend(chunk_word_counts)\n",
    "            \n",
    "            # Progress update every 10 chunks\n",
    "            if chunk_num % 10 == 0:\n",
    "                print(f\"  Processed {chunk_num} chunks ({total_records:,} records)...\")\n",
    "        \n",
    "        print(f\"\\nCompleted! Processed {chunk_num} chunks total.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found: {filepath}\")\n",
    "        print(f\"Please update the base_path variable to point to your MIMIC-IV data directory.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR analyzing file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_records': total_records,\n",
    "        'unique_subjects': len(unique_subjects),\n",
    "        'unique_hadms': len(unique_hadms),\n",
    "        'word_counts': word_counts\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{file_label} Statistics:\")\n",
    "    print(f\"  Total records: {stats['total_records']:,}\")\n",
    "    print(f\"  Unique subject_id: {stats['unique_subjects']:,}\")\n",
    "    print(f\"  Unique hadm_id: {stats['unique_hadms']:,}\")\n",
    "    \n",
    "    if word_counts:\n",
    "        word_counts_array = np.array(word_counts)\n",
    "        print(f\"\\n  Word Count Statistics:\")\n",
    "        print(f\"    Mean: {word_counts_array.mean():.2f}\")\n",
    "        print(f\"    Median: {np.median(word_counts_array):.2f}\")\n",
    "        print(f\"    Max: {word_counts_array.max():,}\")\n",
    "        print(f\"    95th Percentile: {np.percentile(word_counts_array, 95):.2f}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Function defined: analyze_file_with_chunks()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update file paths to use .csv.gz format\n",
    "# Adjust the base_path to match your MIMIC-IV data location\n",
    "base_path = 'C:\\\\Users\\\\Eli\\\\Data\\\\physionet.org\\\\files\\\\mimic-iv-note\\\\2.2\\\\Unzipped\\\\'\n",
    "\n",
    "# Clinical text files (now using .csv.gz format)\n",
    "discharge_file = base_path + 'discharge.csv.gz'\n",
    "radiology_file = base_path + 'radiology.csv.gz'\n",
    "\n",
    "# Metadata files\n",
    "discharge_detail_file = base_path + 'discharge_detail.csv.gz'\n",
    "radiology_detail_file = base_path + 'radiology_detail.csv.gz'\n",
    "\n",
    "print(\"File paths configured for .csv.gz format:\")\n",
    "print(f\"  Discharge: {discharge_file}\")\n",
    "print(f\"  Discharge Detail: {discharge_detail_file}\")\n",
    "print(f\"  Radiology: {radiology_file}\")\n",
    "print(f\"  Radiology Detail: {radiology_detail_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze discharge.csv.gz\n",
    "discharge_stats = analyze_file_with_chunks(discharge_file, 'Discharge Notes', chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze discharge_detail.csv.gz\n",
    "discharge_detail_stats = analyze_file_with_chunks(discharge_detail_file, 'Discharge Detail', chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze radiology.csv.gz\n",
    "radiology_stats = analyze_file_with_chunks(radiology_file, 'Radiology Notes', chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze radiology_detail.csv.gz\n",
    "radiology_detail_stats = analyze_file_with_chunks(radiology_detail_file, 'Radiology Detail', chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Histogram of Word Count Distribution (Discharge vs Radiology)\n",
    "print(\"\\nCreating word count distribution histogram...\")\n",
    "\n",
    "if discharge_stats and radiology_stats and discharge_stats['word_counts'] and radiology_stats['word_counts']:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    \n",
    "    # Plot histograms\n",
    "    ax.hist(discharge_stats['word_counts'], bins=100, alpha=0.6, label='Discharge', color='steelblue', edgecolor='black')\n",
    "    ax.hist(radiology_stats['word_counts'], bins=100, alpha=0.6, label='Radiology', color='coral', edgecolor='black')\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax.axvline(x=512, color='red', linestyle='--', linewidth=2, label='BERT Limit (512 words)')\n",
    "    \n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel('Word Count', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title('Word Count Distribution: Discharge vs Radiology Notes (Full Dataset)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Use log scale if distribution is highly skewed\n",
    "    # Uncomment the next line if needed:\n",
    "    # ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHistogram created successfully!\")\n",
    "else:\n",
    "    print(\"\\nSkipping histogram: Data not available or file analysis failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sample Embeddings with Clinical-ModernBERT\n",
    "\n",
    "This section generates embeddings for a small sample of notes using the Clinical-ModernBERT model.\n",
    "\n",
    "**Key points:**\n",
    "- Load **all records** from discharge.csv.gz and radiology.csv.gz\n",
    "- Generate embeddings for only the **first 50 records** from each dataset\n",
    "- Use **Mean Pooling** of the last_hidden_state (NOT pooler_output)\n",
    "- Save results as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for embeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the Clinical ModernBERT model and tokenizer\n",
    "model_name = \"Simonlee711/Clinical_ModernBERT\"\n",
    "print(f\"\\nLoading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Perform mean pooling on the last_hidden_state.\n",
    "    \n",
    "    Logic: sum(token_embeddings * attention_mask) / sum(attention_mask)\n",
    "    \n",
    "    Args:\n",
    "        model_output: Output from the model (contains last_hidden_state)\n",
    "        attention_mask: Attention mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        Mean-pooled embeddings\n",
    "    \"\"\"\n",
    "    # Extract last_hidden_state from model output\n",
    "    token_embeddings = model_output.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # Expand attention_mask to match token_embeddings dimensions\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    # Sum token embeddings weighted by attention mask\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    \n",
    "    # Sum attention mask\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "    \n",
    "    # Calculate mean\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    return mean_embeddings\n",
    "\n",
    "def get_embeddings_with_mean_pooling(texts, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using mean pooling.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                                 max_length=8192, return_tensors='pt')\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        # Generate embeddings (no gradient calculation needed)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        # Apply mean pooling\n",
    "        embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"Embedding functions defined:\")\n",
    "print(\"  - mean_pooling()\")\n",
    "print(\"  - get_embeddings_with_mean_pooling()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all records from discharge.csv.gz\n",
    "print(\"Loading all records from discharge.csv.gz...\")\n",
    "discharge_sample = pd.read_csv(discharge_file, compression='gzip')\n",
    "print(f\"Loaded {len(discharge_sample)} discharge notes\")\n",
    "print(f\"Columns: {list(discharge_sample.columns)}\")\n",
    "\n",
    "# Load all records from radiology.csv.gz\n",
    "print(\"\\nLoading all records from radiology.csv.gz...\")\n",
    "radiology_sample = pd.read_csv(radiology_file, compression='gzip')\n",
    "print(f\"Loaded {len(radiology_sample)} radiology notes\")\n",
    "print(f\"Columns: {list(radiology_sample.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for first 50 discharge records\n",
    "print(\"\\nGenerating embeddings for first 50 discharge records...\")\n",
    "discharge_texts = discharge_sample['text'].head(50).fillna('').tolist()\n",
    "discharge_embeddings = get_embeddings_with_mean_pooling(discharge_texts, batch_size=8)\n",
    "print(f\"Generated {len(discharge_embeddings)} embeddings with shape: {discharge_embeddings.shape}\")\n",
    "\n",
    "# Create DataFrame with required columns\n",
    "discharge_sample_emb = pd.DataFrame({\n",
    "    'note_id': discharge_sample['note_id'].head(50).values,\n",
    "    'subject_id': discharge_sample['subject_id'].head(50).values,\n",
    "    'hadm_id': discharge_sample['hadm_id'].head(50).values,\n",
    "    'note_embedding': list(discharge_embeddings)\n",
    "})\n",
    "\n",
    "print(f\"\\nCreated discharge_sample_emb DataFrame:\")\n",
    "print(f\"  Shape: {discharge_sample_emb.shape}\")\n",
    "print(f\"  Columns: {list(discharge_sample_emb.columns)}\")\n",
    "print(f\"  Embedding dimension: {len(discharge_sample_emb['note_embedding'].iloc[0])}\")\n",
    "\n",
    "# Save to parquet\n",
    "output_file = 'discharge_50_embeddings.parquet'\n",
    "discharge_sample_emb.to_parquet(output_file, index=False)\n",
    "print(f\"\\nSaved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for first 50 radiology records\n",
    "print(\"\\nGenerating embeddings for first 50 radiology records...\")\n",
    "radiology_texts = radiology_sample['text'].head(50).fillna('').tolist()\n",
    "radiology_embeddings = get_embeddings_with_mean_pooling(radiology_texts, batch_size=8)\n",
    "print(f\"Generated {len(radiology_embeddings)} embeddings with shape: {radiology_embeddings.shape}\")\n",
    "\n",
    "# Create DataFrame with required columns\n",
    "radiology_sample_emb = pd.DataFrame({\n",
    "    'note_id': radiology_sample['note_id'].head(50).values,\n",
    "    'subject_id': radiology_sample['subject_id'].head(50).values,\n",
    "    'hadm_id': radiology_sample['hadm_id'].head(50).values,\n",
    "    'note_embedding': list(radiology_embeddings)\n",
    "})\n",
    "\n",
    "print(f\"\\nCreated radiology_sample_emb DataFrame:\")\n",
    "print(f\"  Shape: {radiology_sample_emb.shape}\")\n",
    "print(f\"  Columns: {list(radiology_sample_emb.columns)}\")\n",
    "print(f\"  Embedding dimension: {len(radiology_sample_emb['note_embedding'].iloc[0])}\")\n",
    "\n",
    "# Save to parquet\n",
    "output_file = 'radiology_50_embeddings.parquet'\n",
    "radiology_sample_emb.to_parquet(output_file, index=False)\n",
    "print(f\"\\nSaved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has completed:\n",
    "\n",
    "### Part 1: Full Dataset Analysis\n",
    "- Analyzed complete datasets using chunking (10,000 rows per chunk)\n",
    "- Calculated total records, unique IDs, and word count statistics\n",
    "- Generated histogram comparing discharge and radiology note lengths\n",
    "\n",
    "### Part 2: Sample Embeddings\n",
    "- Loaded all records from discharge and radiology datasets\n",
    "- Generated embeddings for 50 discharge notes and 50 radiology notes\n",
    "- Used Clinical-ModernBERT with **mean pooling** strategy\n",
    "- Saved embeddings to parquet files:\n",
    "  - `discharge_50_embeddings.parquet`\n",
    "  - `radiology_50_embeddings.parquet`\n",
    "\n",
    "### Next Steps\n",
    "- Review word count statistics to determine optimal model architecture\n",
    "- Use the sample embeddings for downstream tasks (e.g., similarity analysis, clustering)\n",
    "- Scale up embedding generation if needed using the same chunking approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}