{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Events Feature Extraction for MIMIC-IV\n",
    "\n",
    "This notebook extracts a rich feature list from the ICU chartevents table, which is the largest table in MIMIC-IV. We identify all unique items (features) available and capture sample values (numeric vs text) and units for each item to decide on the embedding strategy (Normalization vs BioBERT) later.\n",
    "\n",
    "Since chartevents is too large to load into RAM at once, we use a chunk processing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T08:54:44.558190600Z",
     "start_time": "2026-01-24T08:54:44.209486200Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define File Paths\n",
    "\n",
    "Define placeholders for the data files. Update these paths to match your data location."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T08:57:54.574566Z",
     "start_time": "2026-01-24T08:57:54.559597600Z"
    }
   },
   "source": [
    "# Define file paths - update these to match your data location\n",
    "chartevents_file_path = 'C:\\\\Users\\\\Eli\\\\Data\\\\physionet.org\\\\files\\\\mimiciv\\\\3.1\\\\icu\\\\chartevents.csv.gz'  # Large file containing ICU chart events\n",
    "d_items_file_path = 'C:\\\\Users\\\\Eli\\\\Data\\\\physionet.org\\\\files\\\\mimiciv\\\\3.1\\\\icu\\\\d_items.csv.gz'  # Dictionary file with item metadata\n",
    "output_file_path = '../data/features/features_chartevents.csv'  # Output file for features"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Unique Items with Sample Data (Chunk Processing)\n",
    "\n",
    "Since chartevents is too large to load into RAM, we read it in chunks, extract unique itemids with sample values, and consolidate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:04:06.947439300Z",
     "start_time": "2026-01-24T08:57:57.220828Z"
    }
   },
   "source": [
    "# Initialize an empty list to hold unique features from each chunk\n",
    "print(\"Reading chartevents file in chunks...\")\n",
    "unique_features_list = []\n",
    "\n",
    "# Read chartevents in chunks of 1,000,000 rows at a time\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_csv(\n",
    "    chartevents_file_path,\n",
    "    usecols=['itemid', 'value', 'valuenum', 'valueuom', 'warning'],\n",
    "    chunksize=1000000\n",
    "):\n",
    "    chunk_number += 1\n",
    "    print(f\"Processing chunk {chunk_number}...\")\n",
    "    \n",
    "    # Drop duplicate itemids within this chunk, keeping first occurrence as a sample\n",
    "    chunk_unique = chunk.drop_duplicates(subset=['itemid'], keep='first')\n",
    "    \n",
    "    # Append to our list\n",
    "    unique_features_list.append(chunk_unique)\n",
    "    \n",
    "    print(f\"  Chunk {chunk_number}: {len(chunk_unique)} unique items extracted\")\n",
    "\n",
    "print(f\"\\nTotal chunks processed: {chunk_number}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chartevents file in chunks...\n",
      "Processing chunk 1...\n",
      "  Chunk 1: 1448 unique items extracted\n",
      "Processing chunk 2...\n",
      "  Chunk 2: 1512 unique items extracted\n",
      "Processing chunk 3...\n",
      "  Chunk 3: 1517 unique items extracted\n",
      "Processing chunk 4...\n",
      "  Chunk 4: 1544 unique items extracted\n",
      "Processing chunk 5...\n",
      "  Chunk 5: 1414 unique items extracted\n",
      "Processing chunk 6...\n",
      "  Chunk 6: 1514 unique items extracted\n",
      "Processing chunk 7...\n",
      "  Chunk 7: 1396 unique items extracted\n",
      "Processing chunk 8...\n",
      "  Chunk 8: 1439 unique items extracted\n",
      "Processing chunk 9...\n",
      "  Chunk 9: 1509 unique items extracted\n",
      "Processing chunk 10...\n",
      "  Chunk 10: 1534 unique items extracted\n",
      "Processing chunk 11...\n",
      "  Chunk 11: 1448 unique items extracted\n",
      "Processing chunk 12...\n",
      "  Chunk 12: 1490 unique items extracted\n",
      "Processing chunk 13...\n",
      "  Chunk 13: 1438 unique items extracted\n",
      "Processing chunk 14...\n",
      "  Chunk 14: 1540 unique items extracted\n",
      "Processing chunk 15...\n",
      "  Chunk 15: 1490 unique items extracted\n",
      "Processing chunk 16...\n",
      "  Chunk 16: 1427 unique items extracted\n",
      "Processing chunk 17...\n",
      "  Chunk 17: 1394 unique items extracted\n",
      "Processing chunk 18...\n",
      "  Chunk 18: 1409 unique items extracted\n",
      "Processing chunk 19...\n",
      "  Chunk 19: 1485 unique items extracted\n",
      "Processing chunk 20...\n",
      "  Chunk 20: 1520 unique items extracted\n",
      "Processing chunk 21...\n",
      "  Chunk 21: 1463 unique items extracted\n",
      "Processing chunk 22...\n",
      "  Chunk 22: 1543 unique items extracted\n",
      "Processing chunk 23...\n",
      "  Chunk 23: 1436 unique items extracted\n",
      "Processing chunk 24...\n",
      "  Chunk 24: 1506 unique items extracted\n",
      "Processing chunk 25...\n",
      "  Chunk 25: 1549 unique items extracted\n",
      "Processing chunk 26...\n",
      "  Chunk 26: 1544 unique items extracted\n",
      "Processing chunk 27...\n",
      "  Chunk 27: 1387 unique items extracted\n",
      "Processing chunk 28...\n",
      "  Chunk 28: 1484 unique items extracted\n",
      "Processing chunk 29...\n",
      "  Chunk 29: 1437 unique items extracted\n",
      "Processing chunk 30...\n",
      "  Chunk 30: 1438 unique items extracted\n",
      "Processing chunk 31...\n",
      "  Chunk 31: 1497 unique items extracted\n",
      "Processing chunk 32...\n",
      "  Chunk 32: 1488 unique items extracted\n",
      "Processing chunk 33...\n",
      "  Chunk 33: 1452 unique items extracted\n",
      "Processing chunk 34...\n",
      "  Chunk 34: 1555 unique items extracted\n",
      "Processing chunk 35...\n",
      "  Chunk 35: 1406 unique items extracted\n",
      "Processing chunk 36...\n",
      "  Chunk 36: 1425 unique items extracted\n",
      "Processing chunk 37...\n",
      "  Chunk 37: 1431 unique items extracted\n",
      "Processing chunk 38...\n",
      "  Chunk 38: 1435 unique items extracted\n",
      "Processing chunk 39...\n",
      "  Chunk 39: 1495 unique items extracted\n",
      "Processing chunk 40...\n",
      "  Chunk 40: 1453 unique items extracted\n",
      "Processing chunk 41...\n",
      "  Chunk 41: 1482 unique items extracted\n",
      "Processing chunk 42...\n",
      "  Chunk 42: 1548 unique items extracted\n",
      "Processing chunk 43...\n",
      "  Chunk 43: 1506 unique items extracted\n",
      "Processing chunk 44...\n",
      "  Chunk 44: 1493 unique items extracted\n",
      "Processing chunk 45...\n",
      "  Chunk 45: 1525 unique items extracted\n",
      "Processing chunk 46...\n",
      "  Chunk 46: 1485 unique items extracted\n",
      "Processing chunk 47...\n",
      "  Chunk 47: 1573 unique items extracted\n",
      "Processing chunk 48...\n",
      "  Chunk 48: 1458 unique items extracted\n",
      "Processing chunk 49...\n",
      "  Chunk 49: 1425 unique items extracted\n",
      "Processing chunk 50...\n",
      "  Chunk 50: 1561 unique items extracted\n",
      "Processing chunk 51...\n",
      "  Chunk 51: 1463 unique items extracted\n",
      "Processing chunk 52...\n",
      "  Chunk 52: 1431 unique items extracted\n",
      "Processing chunk 53...\n",
      "  Chunk 53: 1485 unique items extracted\n",
      "Processing chunk 54...\n",
      "  Chunk 54: 1437 unique items extracted\n",
      "Processing chunk 55...\n",
      "  Chunk 55: 1536 unique items extracted\n",
      "Processing chunk 56...\n",
      "  Chunk 56: 1500 unique items extracted\n",
      "Processing chunk 57...\n",
      "  Chunk 57: 1564 unique items extracted\n",
      "Processing chunk 58...\n",
      "  Chunk 58: 1387 unique items extracted\n",
      "Processing chunk 59...\n",
      "  Chunk 59: 1440 unique items extracted\n",
      "Processing chunk 60...\n",
      "  Chunk 60: 1506 unique items extracted\n",
      "Processing chunk 61...\n",
      "  Chunk 61: 1411 unique items extracted\n",
      "Processing chunk 62...\n",
      "  Chunk 62: 1504 unique items extracted\n",
      "Processing chunk 63...\n",
      "  Chunk 63: 1489 unique items extracted\n",
      "Processing chunk 64...\n",
      "  Chunk 64: 1521 unique items extracted\n",
      "Processing chunk 65...\n",
      "  Chunk 65: 1455 unique items extracted\n",
      "Processing chunk 66...\n",
      "  Chunk 66: 1565 unique items extracted\n",
      "Processing chunk 67...\n",
      "  Chunk 67: 1504 unique items extracted\n",
      "Processing chunk 68...\n",
      "  Chunk 68: 1438 unique items extracted\n",
      "Processing chunk 69...\n",
      "  Chunk 69: 1507 unique items extracted\n",
      "Processing chunk 70...\n",
      "  Chunk 70: 1425 unique items extracted\n",
      "Processing chunk 71...\n",
      "  Chunk 71: 1428 unique items extracted\n",
      "Processing chunk 72...\n",
      "  Chunk 72: 1458 unique items extracted\n",
      "Processing chunk 73...\n",
      "  Chunk 73: 1512 unique items extracted\n",
      "Processing chunk 74...\n",
      "  Chunk 74: 1407 unique items extracted\n",
      "Processing chunk 75...\n",
      "  Chunk 75: 1445 unique items extracted\n",
      "Processing chunk 76...\n",
      "  Chunk 76: 1465 unique items extracted\n",
      "Processing chunk 77...\n",
      "  Chunk 77: 1579 unique items extracted\n",
      "Processing chunk 78...\n",
      "  Chunk 78: 1374 unique items extracted\n",
      "Processing chunk 79...\n",
      "  Chunk 79: 1502 unique items extracted\n",
      "Processing chunk 80...\n",
      "  Chunk 80: 1478 unique items extracted\n",
      "Processing chunk 81...\n",
      "  Chunk 81: 1551 unique items extracted\n",
      "Processing chunk 82...\n",
      "  Chunk 82: 1553 unique items extracted\n",
      "Processing chunk 83...\n",
      "  Chunk 83: 1488 unique items extracted\n",
      "Processing chunk 84...\n",
      "  Chunk 84: 1452 unique items extracted\n",
      "Processing chunk 85...\n",
      "  Chunk 85: 1450 unique items extracted\n",
      "Processing chunk 86...\n",
      "  Chunk 86: 1418 unique items extracted\n",
      "Processing chunk 87...\n",
      "  Chunk 87: 1446 unique items extracted\n",
      "Processing chunk 88...\n",
      "  Chunk 88: 1447 unique items extracted\n",
      "Processing chunk 89...\n",
      "  Chunk 89: 1360 unique items extracted\n",
      "Processing chunk 90...\n",
      "  Chunk 90: 1511 unique items extracted\n",
      "Processing chunk 91...\n",
      "  Chunk 91: 1424 unique items extracted\n",
      "Processing chunk 92...\n",
      "  Chunk 92: 1408 unique items extracted\n",
      "Processing chunk 93...\n",
      "  Chunk 93: 1453 unique items extracted\n",
      "Processing chunk 94...\n",
      "  Chunk 94: 1469 unique items extracted\n",
      "Processing chunk 95...\n",
      "  Chunk 95: 1460 unique items extracted\n",
      "Processing chunk 96...\n",
      "  Chunk 96: 1435 unique items extracted\n",
      "Processing chunk 97...\n",
      "  Chunk 97: 1463 unique items extracted\n",
      "Processing chunk 98...\n",
      "  Chunk 98: 1509 unique items extracted\n",
      "Processing chunk 99...\n",
      "  Chunk 99: 1416 unique items extracted\n",
      "Processing chunk 100...\n",
      "  Chunk 100: 1597 unique items extracted\n",
      "Processing chunk 101...\n",
      "  Chunk 101: 1567 unique items extracted\n",
      "Processing chunk 102...\n",
      "  Chunk 102: 1500 unique items extracted\n",
      "Processing chunk 103...\n",
      "  Chunk 103: 1470 unique items extracted\n",
      "Processing chunk 104...\n",
      "  Chunk 104: 1520 unique items extracted\n",
      "Processing chunk 105...\n",
      "  Chunk 105: 1479 unique items extracted\n",
      "Processing chunk 106...\n",
      "  Chunk 106: 1503 unique items extracted\n",
      "Processing chunk 107...\n",
      "  Chunk 107: 1493 unique items extracted\n",
      "Processing chunk 108...\n",
      "  Chunk 108: 1393 unique items extracted\n",
      "Processing chunk 109...\n",
      "  Chunk 109: 1505 unique items extracted\n",
      "Processing chunk 110...\n",
      "  Chunk 110: 1487 unique items extracted\n",
      "Processing chunk 111...\n",
      "  Chunk 111: 1362 unique items extracted\n",
      "Processing chunk 112...\n",
      "  Chunk 112: 1490 unique items extracted\n",
      "Processing chunk 113...\n",
      "  Chunk 113: 1456 unique items extracted\n",
      "Processing chunk 114...\n",
      "  Chunk 114: 1446 unique items extracted\n",
      "Processing chunk 115...\n",
      "  Chunk 115: 1483 unique items extracted\n",
      "Processing chunk 116...\n",
      "  Chunk 116: 1531 unique items extracted\n",
      "Processing chunk 117...\n",
      "  Chunk 117: 1491 unique items extracted\n",
      "Processing chunk 118...\n",
      "  Chunk 118: 1482 unique items extracted\n",
      "Processing chunk 119...\n",
      "  Chunk 119: 1598 unique items extracted\n",
      "Processing chunk 120...\n",
      "  Chunk 120: 1478 unique items extracted\n",
      "Processing chunk 121...\n",
      "  Chunk 121: 1399 unique items extracted\n",
      "Processing chunk 122...\n",
      "  Chunk 122: 1350 unique items extracted\n",
      "Processing chunk 123...\n",
      "  Chunk 123: 1431 unique items extracted\n",
      "Processing chunk 124...\n",
      "  Chunk 124: 1460 unique items extracted\n",
      "Processing chunk 125...\n",
      "  Chunk 125: 1466 unique items extracted\n",
      "Processing chunk 126...\n",
      "  Chunk 126: 1501 unique items extracted\n",
      "Processing chunk 127...\n",
      "  Chunk 127: 1465 unique items extracted\n",
      "Processing chunk 128...\n",
      "  Chunk 128: 1492 unique items extracted\n",
      "Processing chunk 129...\n",
      "  Chunk 129: 1408 unique items extracted\n",
      "Processing chunk 130...\n",
      "  Chunk 130: 1378 unique items extracted\n",
      "Processing chunk 131...\n",
      "  Chunk 131: 1533 unique items extracted\n",
      "Processing chunk 132...\n",
      "  Chunk 132: 1451 unique items extracted\n",
      "Processing chunk 133...\n",
      "  Chunk 133: 1487 unique items extracted\n",
      "Processing chunk 134...\n",
      "  Chunk 134: 1498 unique items extracted\n",
      "Processing chunk 135...\n",
      "  Chunk 135: 1462 unique items extracted\n",
      "Processing chunk 136...\n",
      "  Chunk 136: 1535 unique items extracted\n",
      "Processing chunk 137...\n",
      "  Chunk 137: 1535 unique items extracted\n",
      "Processing chunk 138...\n",
      "  Chunk 138: 1396 unique items extracted\n",
      "Processing chunk 139...\n",
      "  Chunk 139: 1423 unique items extracted\n",
      "Processing chunk 140...\n",
      "  Chunk 140: 1409 unique items extracted\n",
      "Processing chunk 141...\n",
      "  Chunk 141: 1428 unique items extracted\n",
      "Processing chunk 142...\n",
      "  Chunk 142: 1518 unique items extracted\n",
      "Processing chunk 143...\n",
      "  Chunk 143: 1493 unique items extracted\n",
      "Processing chunk 144...\n",
      "  Chunk 144: 1492 unique items extracted\n",
      "Processing chunk 145...\n",
      "  Chunk 145: 1483 unique items extracted\n",
      "Processing chunk 146...\n",
      "  Chunk 146: 1434 unique items extracted\n",
      "Processing chunk 147...\n",
      "  Chunk 147: 1547 unique items extracted\n",
      "Processing chunk 148...\n",
      "  Chunk 148: 1524 unique items extracted\n",
      "Processing chunk 149...\n",
      "  Chunk 149: 1418 unique items extracted\n",
      "Processing chunk 150...\n",
      "  Chunk 150: 1400 unique items extracted\n",
      "Processing chunk 151...\n",
      "  Chunk 151: 1491 unique items extracted\n",
      "Processing chunk 152...\n",
      "  Chunk 152: 1461 unique items extracted\n",
      "Processing chunk 153...\n",
      "  Chunk 153: 1496 unique items extracted\n",
      "Processing chunk 154...\n",
      "  Chunk 154: 1421 unique items extracted\n",
      "Processing chunk 155...\n",
      "  Chunk 155: 1437 unique items extracted\n",
      "Processing chunk 156...\n",
      "  Chunk 156: 1463 unique items extracted\n",
      "Processing chunk 157...\n",
      "  Chunk 157: 1548 unique items extracted\n",
      "Processing chunk 158...\n",
      "  Chunk 158: 1385 unique items extracted\n",
      "Processing chunk 159...\n",
      "  Chunk 159: 1531 unique items extracted\n",
      "Processing chunk 160...\n",
      "  Chunk 160: 1468 unique items extracted\n",
      "Processing chunk 161...\n",
      "  Chunk 161: 1570 unique items extracted\n",
      "Processing chunk 162...\n",
      "  Chunk 162: 1460 unique items extracted\n",
      "Processing chunk 163...\n",
      "  Chunk 163: 1415 unique items extracted\n",
      "Processing chunk 164...\n",
      "  Chunk 164: 1477 unique items extracted\n",
      "Processing chunk 165...\n",
      "  Chunk 165: 1414 unique items extracted\n",
      "Processing chunk 166...\n",
      "  Chunk 166: 1455 unique items extracted\n",
      "Processing chunk 167...\n",
      "  Chunk 167: 1512 unique items extracted\n",
      "Processing chunk 168...\n",
      "  Chunk 168: 1430 unique items extracted\n",
      "Processing chunk 169...\n",
      "  Chunk 169: 1431 unique items extracted\n",
      "Processing chunk 170...\n",
      "  Chunk 170: 1537 unique items extracted\n",
      "Processing chunk 171...\n",
      "  Chunk 171: 1485 unique items extracted\n",
      "Processing chunk 172...\n",
      "  Chunk 172: 1505 unique items extracted\n",
      "Processing chunk 173...\n",
      "  Chunk 173: 1471 unique items extracted\n",
      "Processing chunk 174...\n",
      "  Chunk 174: 1449 unique items extracted\n",
      "Processing chunk 175...\n",
      "  Chunk 175: 1516 unique items extracted\n",
      "Processing chunk 176...\n",
      "  Chunk 176: 1454 unique items extracted\n",
      "Processing chunk 177...\n",
      "  Chunk 177: 1441 unique items extracted\n",
      "Processing chunk 178...\n",
      "  Chunk 178: 1388 unique items extracted\n",
      "Processing chunk 179...\n",
      "  Chunk 179: 1488 unique items extracted\n",
      "Processing chunk 180...\n",
      "  Chunk 180: 1455 unique items extracted\n",
      "Processing chunk 181...\n",
      "  Chunk 181: 1579 unique items extracted\n",
      "Processing chunk 182...\n",
      "  Chunk 182: 1510 unique items extracted\n",
      "Processing chunk 183...\n",
      "  Chunk 183: 1586 unique items extracted\n",
      "Processing chunk 184...\n",
      "  Chunk 184: 1460 unique items extracted\n",
      "Processing chunk 185...\n",
      "  Chunk 185: 1418 unique items extracted\n",
      "Processing chunk 186...\n",
      "  Chunk 186: 1466 unique items extracted\n",
      "Processing chunk 187...\n",
      "  Chunk 187: 1501 unique items extracted\n",
      "Processing chunk 188...\n",
      "  Chunk 188: 1402 unique items extracted\n",
      "Processing chunk 189...\n",
      "  Chunk 189: 1475 unique items extracted\n",
      "Processing chunk 190...\n",
      "  Chunk 190: 1435 unique items extracted\n",
      "Processing chunk 191...\n",
      "  Chunk 191: 1454 unique items extracted\n",
      "Processing chunk 192...\n",
      "  Chunk 192: 1501 unique items extracted\n",
      "Processing chunk 193...\n",
      "  Chunk 193: 1479 unique items extracted\n",
      "Processing chunk 194...\n",
      "  Chunk 194: 1495 unique items extracted\n",
      "Processing chunk 195...\n",
      "  Chunk 195: 1414 unique items extracted\n",
      "Processing chunk 196...\n",
      "  Chunk 196: 1560 unique items extracted\n",
      "Processing chunk 197...\n",
      "  Chunk 197: 1527 unique items extracted\n",
      "Processing chunk 198...\n",
      "  Chunk 198: 1499 unique items extracted\n",
      "Processing chunk 199...\n",
      "  Chunk 199: 1461 unique items extracted\n",
      "Processing chunk 200...\n",
      "  Chunk 200: 1452 unique items extracted\n",
      "Processing chunk 201...\n",
      "  Chunk 201: 1426 unique items extracted\n",
      "Processing chunk 202...\n",
      "  Chunk 202: 1553 unique items extracted\n",
      "Processing chunk 203...\n",
      "  Chunk 203: 1403 unique items extracted\n",
      "Processing chunk 204...\n",
      "  Chunk 204: 1526 unique items extracted\n",
      "Processing chunk 205...\n",
      "  Chunk 205: 1552 unique items extracted\n",
      "Processing chunk 206...\n",
      "  Chunk 206: 1505 unique items extracted\n",
      "Processing chunk 207...\n",
      "  Chunk 207: 1528 unique items extracted\n",
      "Processing chunk 208...\n",
      "  Chunk 208: 1570 unique items extracted\n",
      "Processing chunk 209...\n",
      "  Chunk 209: 1403 unique items extracted\n",
      "Processing chunk 210...\n",
      "  Chunk 210: 1512 unique items extracted\n",
      "Processing chunk 211...\n",
      "  Chunk 211: 1528 unique items extracted\n",
      "Processing chunk 212...\n",
      "  Chunk 212: 1472 unique items extracted\n",
      "Processing chunk 213...\n",
      "  Chunk 213: 1541 unique items extracted\n",
      "Processing chunk 214...\n",
      "  Chunk 214: 1466 unique items extracted\n",
      "Processing chunk 215...\n",
      "  Chunk 215: 1437 unique items extracted\n",
      "Processing chunk 216...\n",
      "  Chunk 216: 1468 unique items extracted\n",
      "Processing chunk 217...\n",
      "  Chunk 217: 1474 unique items extracted\n",
      "Processing chunk 218...\n",
      "  Chunk 218: 1424 unique items extracted\n",
      "Processing chunk 219...\n",
      "  Chunk 219: 1509 unique items extracted\n",
      "Processing chunk 220...\n",
      "  Chunk 220: 1584 unique items extracted\n",
      "Processing chunk 221...\n",
      "  Chunk 221: 1469 unique items extracted\n",
      "Processing chunk 222...\n",
      "  Chunk 222: 1469 unique items extracted\n",
      "Processing chunk 223...\n",
      "  Chunk 223: 1491 unique items extracted\n",
      "Processing chunk 224...\n",
      "  Chunk 224: 1534 unique items extracted\n",
      "Processing chunk 225...\n",
      "  Chunk 225: 1400 unique items extracted\n",
      "Processing chunk 226...\n",
      "  Chunk 226: 1469 unique items extracted\n",
      "Processing chunk 227...\n",
      "  Chunk 227: 1486 unique items extracted\n",
      "Processing chunk 228...\n",
      "  Chunk 228: 1441 unique items extracted\n",
      "Processing chunk 229...\n",
      "  Chunk 229: 1497 unique items extracted\n",
      "Processing chunk 230...\n",
      "  Chunk 230: 1358 unique items extracted\n",
      "Processing chunk 231...\n",
      "  Chunk 231: 1445 unique items extracted\n",
      "Processing chunk 232...\n",
      "  Chunk 232: 1492 unique items extracted\n",
      "Processing chunk 233...\n",
      "  Chunk 233: 1582 unique items extracted\n",
      "Processing chunk 234...\n",
      "  Chunk 234: 1430 unique items extracted\n",
      "Processing chunk 235...\n",
      "  Chunk 235: 1467 unique items extracted\n",
      "Processing chunk 236...\n",
      "  Chunk 236: 1537 unique items extracted\n",
      "Processing chunk 237...\n",
      "  Chunk 237: 1513 unique items extracted\n",
      "Processing chunk 238...\n",
      "  Chunk 238: 1508 unique items extracted\n",
      "Processing chunk 239...\n",
      "  Chunk 239: 1465 unique items extracted\n",
      "Processing chunk 240...\n",
      "  Chunk 240: 1389 unique items extracted\n",
      "Processing chunk 241...\n",
      "  Chunk 241: 1614 unique items extracted\n",
      "Processing chunk 242...\n",
      "  Chunk 242: 1419 unique items extracted\n",
      "Processing chunk 243...\n",
      "  Chunk 243: 1428 unique items extracted\n",
      "Processing chunk 244...\n",
      "  Chunk 244: 1447 unique items extracted\n",
      "Processing chunk 245...\n",
      "  Chunk 245: 1497 unique items extracted\n",
      "Processing chunk 246...\n",
      "  Chunk 246: 1520 unique items extracted\n",
      "Processing chunk 247...\n",
      "  Chunk 247: 1557 unique items extracted\n",
      "Processing chunk 248...\n",
      "  Chunk 248: 1370 unique items extracted\n",
      "Processing chunk 249...\n",
      "  Chunk 249: 1523 unique items extracted\n",
      "Processing chunk 250...\n",
      "  Chunk 250: 1500 unique items extracted\n",
      "Processing chunk 251...\n",
      "  Chunk 251: 1414 unique items extracted\n",
      "Processing chunk 252...\n",
      "  Chunk 252: 1412 unique items extracted\n",
      "Processing chunk 253...\n",
      "  Chunk 253: 1396 unique items extracted\n",
      "Processing chunk 254...\n",
      "  Chunk 254: 1487 unique items extracted\n",
      "Processing chunk 255...\n",
      "  Chunk 255: 1433 unique items extracted\n",
      "Processing chunk 256...\n",
      "  Chunk 256: 1468 unique items extracted\n",
      "Processing chunk 257...\n",
      "  Chunk 257: 1504 unique items extracted\n",
      "Processing chunk 258...\n",
      "  Chunk 258: 1435 unique items extracted\n",
      "Processing chunk 259...\n",
      "  Chunk 259: 1476 unique items extracted\n",
      "Processing chunk 260...\n",
      "  Chunk 260: 1525 unique items extracted\n",
      "Processing chunk 261...\n",
      "  Chunk 261: 1438 unique items extracted\n",
      "Processing chunk 262...\n",
      "  Chunk 262: 1519 unique items extracted\n",
      "Processing chunk 263...\n",
      "  Chunk 263: 1487 unique items extracted\n",
      "Processing chunk 264...\n",
      "  Chunk 264: 1517 unique items extracted\n",
      "Processing chunk 265...\n",
      "  Chunk 265: 1441 unique items extracted\n",
      "Processing chunk 266...\n",
      "  Chunk 266: 1529 unique items extracted\n",
      "Processing chunk 267...\n",
      "  Chunk 267: 1400 unique items extracted\n",
      "Processing chunk 268...\n",
      "  Chunk 268: 1470 unique items extracted\n",
      "Processing chunk 269...\n",
      "  Chunk 269: 1559 unique items extracted\n",
      "Processing chunk 270...\n",
      "  Chunk 270: 1517 unique items extracted\n",
      "Processing chunk 271...\n",
      "  Chunk 271: 1501 unique items extracted\n",
      "Processing chunk 272...\n",
      "  Chunk 272: 1516 unique items extracted\n",
      "Processing chunk 273...\n",
      "  Chunk 273: 1419 unique items extracted\n",
      "Processing chunk 274...\n",
      "  Chunk 274: 1477 unique items extracted\n",
      "Processing chunk 275...\n",
      "  Chunk 275: 1526 unique items extracted\n",
      "Processing chunk 276...\n",
      "  Chunk 276: 1395 unique items extracted\n",
      "Processing chunk 277...\n",
      "  Chunk 277: 1496 unique items extracted\n",
      "Processing chunk 278...\n",
      "  Chunk 278: 1504 unique items extracted\n",
      "Processing chunk 279...\n",
      "  Chunk 279: 1523 unique items extracted\n",
      "Processing chunk 280...\n",
      "  Chunk 280: 1607 unique items extracted\n",
      "Processing chunk 281...\n",
      "  Chunk 281: 1388 unique items extracted\n",
      "Processing chunk 282...\n",
      "  Chunk 282: 1496 unique items extracted\n",
      "Processing chunk 283...\n",
      "  Chunk 283: 1456 unique items extracted\n",
      "Processing chunk 284...\n",
      "  Chunk 284: 1463 unique items extracted\n",
      "Processing chunk 285...\n",
      "  Chunk 285: 1527 unique items extracted\n",
      "Processing chunk 286...\n",
      "  Chunk 286: 1435 unique items extracted\n",
      "Processing chunk 287...\n",
      "  Chunk 287: 1528 unique items extracted\n",
      "Processing chunk 288...\n",
      "  Chunk 288: 1433 unique items extracted\n",
      "Processing chunk 289...\n",
      "  Chunk 289: 1435 unique items extracted\n",
      "Processing chunk 290...\n",
      "  Chunk 290: 1468 unique items extracted\n",
      "Processing chunk 291...\n",
      "  Chunk 291: 1487 unique items extracted\n",
      "Processing chunk 292...\n",
      "  Chunk 292: 1398 unique items extracted\n",
      "Processing chunk 293...\n",
      "  Chunk 293: 1563 unique items extracted\n",
      "Processing chunk 294...\n",
      "  Chunk 294: 1422 unique items extracted\n",
      "Processing chunk 295...\n",
      "  Chunk 295: 1483 unique items extracted\n",
      "Processing chunk 296...\n",
      "  Chunk 296: 1488 unique items extracted\n",
      "Processing chunk 297...\n",
      "  Chunk 297: 1449 unique items extracted\n",
      "Processing chunk 298...\n",
      "  Chunk 298: 1574 unique items extracted\n",
      "Processing chunk 299...\n",
      "  Chunk 299: 1521 unique items extracted\n",
      "Processing chunk 300...\n",
      "  Chunk 300: 1482 unique items extracted\n",
      "Processing chunk 301...\n",
      "  Chunk 301: 1438 unique items extracted\n",
      "Processing chunk 302...\n",
      "  Chunk 302: 1383 unique items extracted\n",
      "Processing chunk 303...\n",
      "  Chunk 303: 1438 unique items extracted\n",
      "Processing chunk 304...\n",
      "  Chunk 304: 1422 unique items extracted\n",
      "Processing chunk 305...\n",
      "  Chunk 305: 1412 unique items extracted\n",
      "Processing chunk 306...\n",
      "  Chunk 306: 1496 unique items extracted\n",
      "Processing chunk 307...\n",
      "  Chunk 307: 1483 unique items extracted\n",
      "Processing chunk 308...\n",
      "  Chunk 308: 1562 unique items extracted\n",
      "Processing chunk 309...\n",
      "  Chunk 309: 1455 unique items extracted\n",
      "Processing chunk 310...\n",
      "  Chunk 310: 1440 unique items extracted\n",
      "Processing chunk 311...\n",
      "  Chunk 311: 1435 unique items extracted\n",
      "Processing chunk 312...\n",
      "  Chunk 312: 1541 unique items extracted\n",
      "Processing chunk 313...\n",
      "  Chunk 313: 1421 unique items extracted\n",
      "Processing chunk 314...\n",
      "  Chunk 314: 1525 unique items extracted\n",
      "Processing chunk 315...\n",
      "  Chunk 315: 1474 unique items extracted\n",
      "Processing chunk 316...\n",
      "  Chunk 316: 1421 unique items extracted\n",
      "Processing chunk 317...\n",
      "  Chunk 317: 1452 unique items extracted\n",
      "Processing chunk 318...\n",
      "  Chunk 318: 1459 unique items extracted\n",
      "Processing chunk 319...\n",
      "  Chunk 319: 1524 unique items extracted\n",
      "Processing chunk 320...\n",
      "  Chunk 320: 1406 unique items extracted\n",
      "Processing chunk 321...\n",
      "  Chunk 321: 1502 unique items extracted\n",
      "Processing chunk 322...\n",
      "  Chunk 322: 1506 unique items extracted\n",
      "Processing chunk 323...\n",
      "  Chunk 323: 1469 unique items extracted\n",
      "Processing chunk 324...\n",
      "  Chunk 324: 1502 unique items extracted\n",
      "Processing chunk 325...\n",
      "  Chunk 325: 1534 unique items extracted\n",
      "Processing chunk 326...\n",
      "  Chunk 326: 1444 unique items extracted\n",
      "Processing chunk 327...\n",
      "  Chunk 327: 1475 unique items extracted\n",
      "Processing chunk 328...\n",
      "  Chunk 328: 1472 unique items extracted\n",
      "Processing chunk 329...\n",
      "  Chunk 329: 1487 unique items extracted\n",
      "Processing chunk 330...\n",
      "  Chunk 330: 1526 unique items extracted\n",
      "Processing chunk 331...\n",
      "  Chunk 331: 1425 unique items extracted\n",
      "Processing chunk 332...\n",
      "  Chunk 332: 1470 unique items extracted\n",
      "Processing chunk 333...\n",
      "  Chunk 333: 1398 unique items extracted\n",
      "Processing chunk 334...\n",
      "  Chunk 334: 1478 unique items extracted\n",
      "Processing chunk 335...\n",
      "  Chunk 335: 1496 unique items extracted\n",
      "Processing chunk 336...\n",
      "  Chunk 336: 1413 unique items extracted\n",
      "Processing chunk 337...\n",
      "  Chunk 337: 1486 unique items extracted\n",
      "Processing chunk 338...\n",
      "  Chunk 338: 1440 unique items extracted\n",
      "Processing chunk 339...\n",
      "  Chunk 339: 1499 unique items extracted\n",
      "Processing chunk 340...\n",
      "  Chunk 340: 1459 unique items extracted\n",
      "Processing chunk 341...\n",
      "  Chunk 341: 1457 unique items extracted\n",
      "Processing chunk 342...\n",
      "  Chunk 342: 1519 unique items extracted\n",
      "Processing chunk 343...\n",
      "  Chunk 343: 1443 unique items extracted\n",
      "Processing chunk 344...\n",
      "  Chunk 344: 1604 unique items extracted\n",
      "Processing chunk 345...\n",
      "  Chunk 345: 1461 unique items extracted\n",
      "Processing chunk 346...\n",
      "  Chunk 346: 1464 unique items extracted\n",
      "Processing chunk 347...\n",
      "  Chunk 347: 1392 unique items extracted\n",
      "Processing chunk 348...\n",
      "  Chunk 348: 1446 unique items extracted\n",
      "Processing chunk 349...\n",
      "  Chunk 349: 1477 unique items extracted\n",
      "Processing chunk 350...\n",
      "  Chunk 350: 1506 unique items extracted\n",
      "Processing chunk 351...\n",
      "  Chunk 351: 1520 unique items extracted\n",
      "Processing chunk 352...\n",
      "  Chunk 352: 1463 unique items extracted\n",
      "Processing chunk 353...\n",
      "  Chunk 353: 1477 unique items extracted\n",
      "Processing chunk 354...\n",
      "  Chunk 354: 1420 unique items extracted\n",
      "Processing chunk 355...\n",
      "  Chunk 355: 1505 unique items extracted\n",
      "Processing chunk 356...\n",
      "  Chunk 356: 1417 unique items extracted\n",
      "Processing chunk 357...\n",
      "  Chunk 357: 1524 unique items extracted\n",
      "Processing chunk 358...\n",
      "  Chunk 358: 1448 unique items extracted\n",
      "Processing chunk 359...\n",
      "  Chunk 359: 1546 unique items extracted\n",
      "Processing chunk 360...\n",
      "  Chunk 360: 1464 unique items extracted\n",
      "Processing chunk 361...\n",
      "  Chunk 361: 1444 unique items extracted\n",
      "Processing chunk 362...\n",
      "  Chunk 362: 1516 unique items extracted\n",
      "Processing chunk 363...\n",
      "  Chunk 363: 1422 unique items extracted\n",
      "Processing chunk 364...\n",
      "  Chunk 364: 1462 unique items extracted\n",
      "Processing chunk 365...\n",
      "  Chunk 365: 1477 unique items extracted\n",
      "Processing chunk 366...\n",
      "  Chunk 366: 1518 unique items extracted\n",
      "Processing chunk 367...\n",
      "  Chunk 367: 1563 unique items extracted\n",
      "Processing chunk 368...\n",
      "  Chunk 368: 1519 unique items extracted\n",
      "Processing chunk 369...\n",
      "  Chunk 369: 1538 unique items extracted\n",
      "Processing chunk 370...\n",
      "  Chunk 370: 1473 unique items extracted\n",
      "Processing chunk 371...\n",
      "  Chunk 371: 1421 unique items extracted\n",
      "Processing chunk 372...\n",
      "  Chunk 372: 1491 unique items extracted\n",
      "Processing chunk 373...\n",
      "  Chunk 373: 1380 unique items extracted\n",
      "Processing chunk 374...\n",
      "  Chunk 374: 1456 unique items extracted\n",
      "Processing chunk 375...\n",
      "  Chunk 375: 1491 unique items extracted\n",
      "Processing chunk 376...\n",
      "  Chunk 376: 1435 unique items extracted\n",
      "Processing chunk 377...\n",
      "  Chunk 377: 1521 unique items extracted\n",
      "Processing chunk 378...\n",
      "  Chunk 378: 1499 unique items extracted\n",
      "Processing chunk 379...\n",
      "  Chunk 379: 1480 unique items extracted\n",
      "Processing chunk 380...\n",
      "  Chunk 380: 1433 unique items extracted\n",
      "Processing chunk 381...\n",
      "  Chunk 381: 1506 unique items extracted\n",
      "Processing chunk 382...\n",
      "  Chunk 382: 1441 unique items extracted\n",
      "Processing chunk 383...\n",
      "  Chunk 383: 1448 unique items extracted\n",
      "Processing chunk 384...\n",
      "  Chunk 384: 1437 unique items extracted\n",
      "Processing chunk 385...\n",
      "  Chunk 385: 1480 unique items extracted\n",
      "Processing chunk 386...\n",
      "  Chunk 386: 1567 unique items extracted\n",
      "Processing chunk 387...\n",
      "  Chunk 387: 1426 unique items extracted\n",
      "Processing chunk 388...\n",
      "  Chunk 388: 1489 unique items extracted\n",
      "Processing chunk 389...\n",
      "  Chunk 389: 1439 unique items extracted\n",
      "Processing chunk 390...\n",
      "  Chunk 390: 1413 unique items extracted\n",
      "Processing chunk 391...\n",
      "  Chunk 391: 1467 unique items extracted\n",
      "Processing chunk 392...\n",
      "  Chunk 392: 1473 unique items extracted\n",
      "Processing chunk 393...\n",
      "  Chunk 393: 1606 unique items extracted\n",
      "Processing chunk 394...\n",
      "  Chunk 394: 1453 unique items extracted\n",
      "Processing chunk 395...\n",
      "  Chunk 395: 1483 unique items extracted\n",
      "Processing chunk 396...\n",
      "  Chunk 396: 1504 unique items extracted\n",
      "Processing chunk 397...\n",
      "  Chunk 397: 1476 unique items extracted\n",
      "Processing chunk 398...\n",
      "  Chunk 398: 1510 unique items extracted\n",
      "Processing chunk 399...\n",
      "  Chunk 399: 1452 unique items extracted\n",
      "Processing chunk 400...\n",
      "  Chunk 400: 1532 unique items extracted\n",
      "Processing chunk 401...\n",
      "  Chunk 401: 1473 unique items extracted\n",
      "Processing chunk 402...\n",
      "  Chunk 402: 1630 unique items extracted\n",
      "Processing chunk 403...\n",
      "  Chunk 403: 1503 unique items extracted\n",
      "Processing chunk 404...\n",
      "  Chunk 404: 1468 unique items extracted\n",
      "Processing chunk 405...\n",
      "  Chunk 405: 1468 unique items extracted\n",
      "Processing chunk 406...\n",
      "  Chunk 406: 1478 unique items extracted\n",
      "Processing chunk 407...\n",
      "  Chunk 407: 1483 unique items extracted\n",
      "Processing chunk 408...\n",
      "  Chunk 408: 1571 unique items extracted\n",
      "Processing chunk 409...\n",
      "  Chunk 409: 1516 unique items extracted\n",
      "Processing chunk 410...\n",
      "  Chunk 410: 1403 unique items extracted\n",
      "Processing chunk 411...\n",
      "  Chunk 411: 1441 unique items extracted\n",
      "Processing chunk 412...\n",
      "  Chunk 412: 1502 unique items extracted\n",
      "Processing chunk 413...\n",
      "  Chunk 413: 1442 unique items extracted\n",
      "Processing chunk 414...\n",
      "  Chunk 414: 1428 unique items extracted\n",
      "Processing chunk 415...\n",
      "  Chunk 415: 1561 unique items extracted\n",
      "Processing chunk 416...\n",
      "  Chunk 416: 1494 unique items extracted\n",
      "Processing chunk 417...\n",
      "  Chunk 417: 1416 unique items extracted\n",
      "Processing chunk 418...\n",
      "  Chunk 418: 1458 unique items extracted\n",
      "Processing chunk 419...\n",
      "  Chunk 419: 1425 unique items extracted\n",
      "Processing chunk 420...\n",
      "  Chunk 420: 1549 unique items extracted\n",
      "Processing chunk 421...\n",
      "  Chunk 421: 1566 unique items extracted\n",
      "Processing chunk 422...\n",
      "  Chunk 422: 1390 unique items extracted\n",
      "Processing chunk 423...\n",
      "  Chunk 423: 1473 unique items extracted\n",
      "Processing chunk 424...\n",
      "  Chunk 424: 1465 unique items extracted\n",
      "Processing chunk 425...\n",
      "  Chunk 425: 1550 unique items extracted\n",
      "Processing chunk 426...\n",
      "  Chunk 426: 1380 unique items extracted\n",
      "Processing chunk 427...\n",
      "  Chunk 427: 1497 unique items extracted\n",
      "Processing chunk 428...\n",
      "  Chunk 428: 1508 unique items extracted\n",
      "Processing chunk 429...\n",
      "  Chunk 429: 1473 unique items extracted\n",
      "Processing chunk 430...\n",
      "  Chunk 430: 1487 unique items extracted\n",
      "Processing chunk 431...\n",
      "  Chunk 431: 1489 unique items extracted\n",
      "Processing chunk 432...\n",
      "  Chunk 432: 1506 unique items extracted\n",
      "Processing chunk 433...\n",
      "  Chunk 433: 1542 unique items extracted\n",
      "\n",
      "Total chunks processed: 433\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate and Remove Duplicates\n",
    "\n",
    "Concatenate all chunks and perform a final deduplication to ensure one row per unique itemid."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:05:57.872749200Z",
     "start_time": "2026-01-24T09:05:57.781033300Z"
    }
   },
   "source": [
    "# Concatenate all chunks into a single DataFrame\n",
    "print(\"\\nConsolidating chunks...\")\n",
    "consolidated_features = pd.concat(unique_features_list, ignore_index=True)\n",
    "print(f\"Total rows before final deduplication: {len(consolidated_features)}\")\n",
    "\n",
    "# Perform final deduplication to ensure one row per unique itemid\n",
    "unique_features = consolidated_features.drop_duplicates(subset=['itemid'], keep='first')\n",
    "print(f\"Total unique items after final deduplication: {len(unique_features)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consolidating chunks...\n",
      "Total rows before final deduplication: 639234\n",
      "Total unique items after final deduplication: 2311\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merge with Item Metadata\n",
    "\n",
    "Load the d_items dictionary and merge it with our extracted features to add descriptive columns like label, category, and param_type."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:06:08.482326Z",
     "start_time": "2026-01-24T09:06:08.444846800Z"
    }
   },
   "source": [
    "# Read the d_items dictionary\n",
    "print(\"\\nReading d_items dictionary file...\")\n",
    "d_items_df = pd.read_csv(d_items_file_path)\n",
    "print(f\"Total items in d_items dictionary: {len(d_items_df)}\")\n",
    "\n",
    "# Perform inner join on itemid to add metadata\n",
    "print(\"\\nMerging with d_items metadata...\")\n",
    "features_with_metadata = pd.merge(\n",
    "    unique_features,\n",
    "    d_items_df,\n",
    "    on='itemid',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Items after merge: {len(features_with_metadata)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading d_items dictionary file...\n",
      "Total items in d_items dictionary: 4095\n",
      "\n",
      "Merging with d_items metadata...\n",
      "Items after merge: 2311\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Output\n",
    "\n",
    "Save the final DataFrame to features_chartevents.csv."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:06:16.273947300Z",
     "start_time": "2026-01-24T09:06:16.244578900Z"
    }
   },
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_file_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the features to CSV\n",
    "print(f\"\\nSaving features to {output_file_path}...\")\n",
    "features_with_metadata.to_csv(output_file_path, index=False)\n",
    "print(\"Features saved successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving features to ../data/features/features_chartevents.csv...\n",
      "Features saved successfully!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verification\n",
    "\n",
    "Display the first 10 rows to verify we have both metadata (label) and sample data (value/valueuom)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:06:18.608019Z",
     "start_time": "2026-01-24T09:06:18.576662800Z"
    }
   },
   "source": [
    "# Display first 10 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: First 10 rows of the feature list:\")\n",
    "print(\"=\"*80)\n",
    "print(features_with_metadata.head(10))\n",
    "\n",
    "# Display total count\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL NUMBER OF CHART EVENT FEATURES: {len(features_with_metadata)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display column names to show we have both metadata and sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Available columns:\")\n",
    "print(\"=\"*80)\n",
    "print(list(features_with_metadata.columns))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION: First 10 rows of the feature list:\n",
      "================================================================================\n",
      "   itemid              value  valuenum valueuom  warning  \\\n",
      "0  226512               39.4      39.4       kg      0.0   \n",
      "1  226707                 60      60.0     Inch      0.0   \n",
      "2  226730                152     152.0       cm      0.0   \n",
      "3  220048  SR (Sinus Rhythm)       NaN      NaN      0.0   \n",
      "4  224642               Oral       NaN      NaN      0.0   \n",
      "5  224650                NaN       NaN      NaN      0.0   \n",
      "6  223761               98.7      98.7       °F      0.0   \n",
      "7  220179                 84      84.0     mmHg      0.0   \n",
      "8  220180                 48      48.0     mmHg      0.0   \n",
      "9  220181                 56      56.0     mmHg      0.0   \n",
      "\n",
      "                                   label           abbreviation      linksto  \\\n",
      "0                  Admission Weight (Kg)  Admission Weight (Kg)  chartevents   \n",
      "1                                 Height                 Height  chartevents   \n",
      "2                            Height (cm)            Height (cm)  chartevents   \n",
      "3                           Heart Rhythm           Heart Rhythm  chartevents   \n",
      "4                       Temperature Site              Temp Site  chartevents   \n",
      "5                          Ectopy Type 1          Ectopy Type 1  chartevents   \n",
      "6                 Temperature Fahrenheit          Temperature F  chartevents   \n",
      "7   Non Invasive Blood Pressure systolic                   NBPs  chartevents   \n",
      "8  Non Invasive Blood Pressure diastolic                   NBPd  chartevents   \n",
      "9       Non Invasive Blood Pressure mean                   NBPm  chartevents   \n",
      "\n",
      "              category unitname param_type  lownormalvalue  highnormalvalue  \n",
      "0              General       kg    Numeric             NaN              NaN  \n",
      "1              General     Inch    Numeric             NaN              NaN  \n",
      "2              General       cm    Numeric             NaN              NaN  \n",
      "3  Routine Vital Signs      NaN       Text             NaN              NaN  \n",
      "4  Routine Vital Signs      NaN       Text             NaN              NaN  \n",
      "5  Routine Vital Signs      NaN       Text             NaN              NaN  \n",
      "6  Routine Vital Signs       °F    Numeric             NaN              NaN  \n",
      "7  Routine Vital Signs     mmHg    Numeric             NaN              NaN  \n",
      "8  Routine Vital Signs     mmHg    Numeric             NaN              NaN  \n",
      "9  Routine Vital Signs     mmHg    Numeric             NaN              NaN  \n",
      "\n",
      "================================================================================\n",
      "TOTAL NUMBER OF CHART EVENT FEATURES: 2311\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Available columns:\n",
      "================================================================================\n",
      "['itemid', 'value', 'valuenum', 'valueuom', 'warning', 'label', 'abbreviation', 'linksto', 'category', 'unitname', 'param_type', 'lownormalvalue', 'highnormalvalue']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "1. ✅ Imported pandas and defined file paths for chartevents.csv.gz and d_items.csv.gz\n",
    "2. ✅ Implemented chunk processing strategy to handle large chartevents file (chunks of 1,000,000 rows)\n",
    "3. ✅ Extracted unique itemids with sample values (value, valuenum, valueuom, warning) from each chunk\n",
    "4. ✅ Consolidated all chunks and performed final deduplication to ensure one row per itemid\n",
    "5. ✅ Merged with d_items metadata to add descriptive columns (label, category, param_type)\n",
    "6. ✅ Saved the result to features_chartevents.csv\n",
    "7. ✅ Verified output shows both metadata (label) and sample data (value/valueuom)\n",
    "\n",
    "The resulting feature list provides a rich dataset with both metadata and sample values/units, enabling informed decisions about embedding strategies (Normalization for numeric values vs BioBERT for text values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
