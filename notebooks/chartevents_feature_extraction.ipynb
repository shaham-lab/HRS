{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Events Feature Extraction for MIMIC-IV\n",
    "\n",
    "This notebook extracts a rich feature list from the ICU chartevents table, which is the largest table in MIMIC-IV. We identify all unique items (features) available and capture sample values (numeric vs text) and units for each item to decide on the embedding strategy (Normalization vs BioBERT) later.\n",
    "\n",
    "Since chartevents is too large to load into RAM at once, we use a chunk processing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T08:54:44.558190600Z",
     "start_time": "2026-01-24T08:54:44.209486200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define File Paths\n",
    "\n",
    "Define placeholders for the data files. Update these paths to match your data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T08:57:54.574566Z",
     "start_time": "2026-01-24T08:57:54.559597600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define file paths - update these to match your data location\n",
    "base_path = '~/data/physionet.org/files/mimiciv/3.1/icu/'\n",
    "\n",
    "chartevents_file_path = base_path + 'chartevents.csv.gz'  # Large file containing ICU chart events\n",
    "d_items_file_path = base_path + 'd_items.csv.gz'  # Dictionary file with item metadata\n",
    "\n",
    "output_file_path = '../data/features/features_chartevents.csv'  # Output file for features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Unique Items with Sample Data (Chunk Processing)\n",
    "\n",
    "Since chartevents is too large to load into RAM, we read it in chunks, extract unique itemids with sample values, and consolidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:04:06.947439300Z",
     "start_time": "2026-01-24T08:57:57.220828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chartevents file in chunks...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dsi/ekazum/Data/physionet.org/files/mimiciv/3.1/icu/chartevents.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Read chartevents in chunks of 1,000,000 rows at a time\u001b[39;00m\n\u001b[32m      6\u001b[39m chunk_number = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchartevents_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mitemid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvaluenum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalueuom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwarning\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000000\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m:\n\u001b[32m     12\u001b[39m     chunk_number += \u001b[32m1\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hrs/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hrs/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hrs/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hrs/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hrs/lib/python3.12/site-packages/pandas/io/common.py:765\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    763\u001b[39m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[32m    764\u001b[39m         \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m         handle = \u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m    766\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    771\u001b[39m         handle = gzip.GzipFile(\n\u001b[32m    772\u001b[39m             \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[32m    773\u001b[39m             \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    776\u001b[39m             **compression_args,\n\u001b[32m    777\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hrs/lib/python3.12/gzip.py:201\u001b[39m, in \u001b[36mGzipFile.__init__\u001b[39m\u001b[34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         fileobj = \u001b[38;5;28mself\u001b[39m.myfileobj = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    203\u001b[39m         filename = \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/dsi/ekazum/Data/physionet.org/files/mimiciv/3.1/icu/chartevents.csv.gz'"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold unique features from each chunk\n",
    "print(\"Reading chartevents file in chunks...\")\n",
    "unique_features_list = []\n",
    "\n",
    "# Read chartevents in chunks of 1,000,000 rows at a time\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_csv(\n",
    "    chartevents_file_path,\n",
    "    usecols=['itemid', 'value', 'valuenum', 'valueuom', 'warning'],\n",
    "    chunksize=1000000\n",
    "):\n",
    "    chunk_number += 1\n",
    "    print(f\"Processing chunk {chunk_number}...\")\n",
    "    \n",
    "    # Drop duplicate itemids within this chunk, keeping first occurrence as a sample\n",
    "    chunk_unique = chunk.drop_duplicates(subset=['itemid'], keep='first')\n",
    "    \n",
    "    # Append to our list\n",
    "    unique_features_list.append(chunk_unique)\n",
    "    \n",
    "    print(f\"  Chunk {chunk_number}: {len(chunk_unique)} unique items extracted\")\n",
    "\n",
    "print(f\"\\nTotal chunks processed: {chunk_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate and Remove Duplicates\n",
    "\n",
    "Concatenate all chunks and perform a final deduplication to ensure one row per unique itemid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:05:57.872749200Z",
     "start_time": "2026-01-24T09:05:57.781033300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consolidating chunks...\n",
      "Total rows before final deduplication: 639234\n",
      "Total unique items after final deduplication: 2311\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all chunks into a single DataFrame\n",
    "print(\"\\nConsolidating chunks...\")\n",
    "consolidated_features = pd.concat(unique_features_list, ignore_index=True)\n",
    "print(f\"Total rows before final deduplication: {len(consolidated_features)}\")\n",
    "\n",
    "# Perform final deduplication to ensure one row per unique itemid\n",
    "unique_features = consolidated_features.drop_duplicates(subset=['itemid'], keep='first')\n",
    "print(f\"Total unique items after final deduplication: {len(unique_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merge with Item Metadata\n",
    "\n",
    "Load the d_items dictionary and merge it with our extracted features to add descriptive columns like label, category, and param_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:06:08.482326Z",
     "start_time": "2026-01-24T09:06:08.444846800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading d_items dictionary file...\n",
      "Total items in d_items dictionary: 4095\n",
      "\n",
      "Merging with d_items metadata...\n",
      "Items after merge: 2311\n"
     ]
    }
   ],
   "source": [
    "# Read the d_items dictionary\n",
    "print(\"\\nReading d_items dictionary file...\")\n",
    "d_items_df = pd.read_csv(d_items_file_path)\n",
    "print(f\"Total items in d_items dictionary: {len(d_items_df)}\")\n",
    "\n",
    "# Perform inner join on itemid to add metadata\n",
    "print(\"\\nMerging with d_items metadata...\")\n",
    "features_with_metadata = pd.merge(\n",
    "    unique_features,\n",
    "    d_items_df,\n",
    "    on='itemid',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Items after merge: {len(features_with_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Output\n",
    "\n",
    "Save the final DataFrame to features_chartevents.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:06:16.273947300Z",
     "start_time": "2026-01-24T09:06:16.244578900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving features to ../data/features/features_chartevents.csv...\n",
      "Features saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_file_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the features to CSV\n",
    "print(f\"\\nSaving features to {output_file_path}...\")\n",
    "features_with_metadata.to_csv(output_file_path, index=False)\n",
    "print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verification\n",
    "\n",
    "Display the first 10 rows to verify we have both metadata (label) and sample data (value/valueuom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:06:18.608019Z",
     "start_time": "2026-01-24T09:06:18.576662800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION: First 10 rows of the feature list:\n",
      "================================================================================\n",
      "   itemid              value  valuenum valueuom  warning  \\\n",
      "0  226512               39.4      39.4       kg      0.0   \n",
      "1  226707                 60      60.0     Inch      0.0   \n",
      "2  226730                152     152.0       cm      0.0   \n",
      "3  220048  SR (Sinus Rhythm)       NaN      NaN      0.0   \n",
      "4  224642               Oral       NaN      NaN      0.0   \n",
      "5  224650                NaN       NaN      NaN      0.0   \n",
      "6  223761               98.7      98.7       °F      0.0   \n",
      "7  220179                 84      84.0     mmHg      0.0   \n",
      "8  220180                 48      48.0     mmHg      0.0   \n",
      "9  220181                 56      56.0     mmHg      0.0   \n",
      "\n",
      "                                   label           abbreviation      linksto  \\\n",
      "0                  Admission Weight (Kg)  Admission Weight (Kg)  chartevents   \n",
      "1                                 Height                 Height  chartevents   \n",
      "2                            Height (cm)            Height (cm)  chartevents   \n",
      "3                           Heart Rhythm           Heart Rhythm  chartevents   \n",
      "4                       Temperature Site              Temp Site  chartevents   \n",
      "5                          Ectopy Type 1          Ectopy Type 1  chartevents   \n",
      "6                 Temperature Fahrenheit          Temperature F  chartevents   \n",
      "7   Non Invasive Blood Pressure systolic                   NBPs  chartevents   \n",
      "8  Non Invasive Blood Pressure diastolic                   NBPd  chartevents   \n",
      "9       Non Invasive Blood Pressure mean                   NBPm  chartevents   \n",
      "\n",
      "              category unitname param_type  lownormalvalue  highnormalvalue  \n",
      "0              General       kg    Numeric             NaN              NaN  \n",
      "1              General     Inch    Numeric             NaN              NaN  \n",
      "2              General       cm    Numeric             NaN              NaN  \n",
      "3  Routine Vital Signs      NaN       Text             NaN              NaN  \n",
      "4  Routine Vital Signs      NaN       Text             NaN              NaN  \n",
      "5  Routine Vital Signs      NaN       Text             NaN              NaN  \n",
      "6  Routine Vital Signs       °F    Numeric             NaN              NaN  \n",
      "7  Routine Vital Signs     mmHg    Numeric             NaN              NaN  \n",
      "8  Routine Vital Signs     mmHg    Numeric             NaN              NaN  \n",
      "9  Routine Vital Signs     mmHg    Numeric             NaN              NaN  \n",
      "\n",
      "================================================================================\n",
      "TOTAL NUMBER OF CHART EVENT FEATURES: 2311\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Available columns:\n",
      "================================================================================\n",
      "['itemid', 'value', 'valuenum', 'valueuom', 'warning', 'label', 'abbreviation', 'linksto', 'category', 'unitname', 'param_type', 'lownormalvalue', 'highnormalvalue']\n"
     ]
    }
   ],
   "source": [
    "# Display first 10 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: First 10 rows of the feature list:\")\n",
    "print(\"=\"*80)\n",
    "print(features_with_metadata.head(10))\n",
    "\n",
    "# Display total count\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL NUMBER OF CHART EVENT FEATURES: {len(features_with_metadata)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display column names to show we have both metadata and sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Available columns:\")\n",
    "print(\"=\"*80)\n",
    "print(list(features_with_metadata.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "1. ✅ Imported pandas and defined file paths for chartevents.csv.gz and d_items.csv.gz\n",
    "2. ✅ Implemented chunk processing strategy to handle large chartevents file (chunks of 1,000,000 rows)\n",
    "3. ✅ Extracted unique itemids with sample values (value, valuenum, valueuom, warning) from each chunk\n",
    "4. ✅ Consolidated all chunks and performed final deduplication to ensure one row per itemid\n",
    "5. ✅ Merged with d_items metadata to add descriptive columns (label, category, param_type)\n",
    "6. ✅ Saved the result to features_chartevents.csv\n",
    "7. ✅ Verified output shows both metadata (label) and sample data (value/valueuom)\n",
    "\n",
    "The resulting feature list provides a rich dataset with both metadata and sample values/units, enabling informed decisions about embedding strategies (Normalization for numeric values vs BioBERT for text values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
