{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Events Feature Extraction for MIMIC-IV\n",
    "\n",
    "This notebook extracts a rich feature list from the ICU chartevents table, which is the largest table in MIMIC-IV. We identify all unique items (features) available and capture sample values (numeric vs text) and units for each item to decide on the embedding strategy (Normalization vs BioBERT) later.\n",
    "\n",
    "Since chartevents is too large to load into RAM at once, we use a chunk processing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define File Paths\n",
    "\n",
    "Define placeholders for the data files. Update these paths to match your data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths - update these to match your data location\n",
    "chartevents_file_path = 'chartevents.csv.gz'  # Large file containing ICU chart events\n",
    "d_items_file_path = 'd_items.csv.gz'  # Dictionary file with item metadata\n",
    "output_file_path = '../data/features/features_chartevents.csv'  # Output file for features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Unique Items with Sample Data (Chunk Processing)\n",
    "\n",
    "Since chartevents is too large to load into RAM, we read it in chunks, extract unique itemids with sample values, and consolidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold unique features from each chunk\n",
    "print(\"Reading chartevents file in chunks...\")\n",
    "unique_features_list = []\n",
    "\n",
    "# Read chartevents in chunks of 1,000,000 rows at a time\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_csv(\n",
    "    chartevents_file_path,\n",
    "    usecols=['itemid', 'value', 'valuenum', 'valueuom', 'warning'],\n",
    "    chunksize=1000000\n",
    "):\n",
    "    chunk_number += 1\n",
    "    print(f\"Processing chunk {chunk_number}...\")\n",
    "    \n",
    "    # Drop duplicate itemids within this chunk, keeping first occurrence as a sample\n",
    "    chunk_unique = chunk.drop_duplicates(subset=['itemid'], keep='first')\n",
    "    \n",
    "    # Append to our list\n",
    "    unique_features_list.append(chunk_unique)\n",
    "    \n",
    "    print(f\"  Chunk {chunk_number}: {len(chunk_unique)} unique items extracted\")\n",
    "\n",
    "print(f\"\\nTotal chunks processed: {chunk_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Consolidate and Remove Duplicates\n",
    "\n",
    "Concatenate all chunks and perform a final deduplication to ensure one row per unique itemid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all chunks into a single DataFrame\n",
    "print(\"\\nConsolidating chunks...\")\n",
    "consolidated_features = pd.concat(unique_features_list, ignore_index=True)\n",
    "print(f\"Total rows before final deduplication: {len(consolidated_features)}\")\n",
    "\n",
    "# Perform final deduplication to ensure one row per unique itemid\n",
    "unique_features = consolidated_features.drop_duplicates(subset=['itemid'], keep='first')\n",
    "print(f\"Total unique items after final deduplication: {len(unique_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merge with Item Metadata\n",
    "\n",
    "Load the d_items dictionary and merge it with our extracted features to add descriptive columns like label, category, and param_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the d_items dictionary\n",
    "print(\"\\nReading d_items dictionary file...\")\n",
    "d_items_df = pd.read_csv(d_items_file_path)\n",
    "print(f\"Total items in d_items dictionary: {len(d_items_df)}\")\n",
    "\n",
    "# Perform inner join on itemid to add metadata\n",
    "print(\"\\nMerging with d_items metadata...\")\n",
    "features_with_metadata = pd.merge(\n",
    "    unique_features,\n",
    "    d_items_df,\n",
    "    on='itemid',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Items after merge: {len(features_with_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Output\n",
    "\n",
    "Save the final DataFrame to features_chartevents.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_file_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the features to CSV\n",
    "print(f\"\\nSaving features to {output_file_path}...\")\n",
    "features_with_metadata.to_csv(output_file_path, index=False)\n",
    "print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verification\n",
    "\n",
    "Display the first 10 rows to verify we have both metadata (label) and sample data (value/valueuom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: First 10 rows of the feature list:\")\n",
    "print(\"=\"*80)\n",
    "print(features_with_metadata.head(10))\n",
    "\n",
    "# Display total count\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL NUMBER OF CHART EVENT FEATURES: {len(features_with_metadata)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display column names to show we have both metadata and sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Available columns:\")\n",
    "print(\"=\"*80)\n",
    "print(list(features_with_metadata.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "1. ✅ Imported pandas and defined file paths for chartevents.csv.gz and d_items.csv.gz\n",
    "2. ✅ Implemented chunk processing strategy to handle large chartevents file (chunks of 1,000,000 rows)\n",
    "3. ✅ Extracted unique itemids with sample values (value, valuenum, valueuom, warning) from each chunk\n",
    "4. ✅ Consolidated all chunks and performed final deduplication to ensure one row per itemid\n",
    "5. ✅ Merged with d_items metadata to add descriptive columns (label, category, param_type)\n",
    "6. ✅ Saved the result to features_chartevents.csv\n",
    "7. ✅ Verified output shows both metadata (label) and sample data (value/valueuom)\n",
    "\n",
    "The resulting feature list provides a rich dataset with both metadata and sample values/units, enabling informed decisions about embedding strategies (Normalization for numeric values vs BioBERT for text values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
